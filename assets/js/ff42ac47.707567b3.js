"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[2478],{416:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>d,contentTitle:()=>l,default:()=>g,frontMatter:()=>i,metadata:()=>o,toc:()=>s});var t=n(8168),r=(n(6540),n(5680));const i={},l="Reward Feedback Learning (Reward FL)",o={unversionedId:"English/UserGuide/algorithms/Reward_FL",id:"English/UserGuide/algorithms/Reward_FL",title:"Reward Feedback Learning (Reward FL)",description:"Introduction",source:"@site/docs/English/UserGuide/algorithms/Reward_FL.md",sourceDirName:"English/UserGuide/algorithms",slug:"/English/UserGuide/algorithms/Reward_FL",permalink:"/ROLL/docs/English/UserGuide/algorithms/Reward_FL",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/algorithms/Reward_FL.md",tags:[],version:"current",lastUpdatedAt:1761386293,formattedLastUpdatedAt:"Oct 25, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Reinforce++",permalink:"/ROLL/docs/English/UserGuide/algorithms/Reinforce_Plus_Plus"},next:{title:"TOPR (Tapered Off-Policy REINFORCE)",permalink:"/ROLL/docs/English/UserGuide/algorithms/TOPR"}},d={},s=[{value:"Introduction",id:"introduction",level:2},{value:"Reward FL Configuration Parameters",id:"reward-fl-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"Wan2_2 Related Parameters",id:"wan2_2-related-parameters",level:3},{value:"Note",id:"note",level:2},{value:"Refernece Model",id:"refernece-model",level:2},{value:"Reference Example",id:"reference-example",level:2}],p={toc:s},m="wrapper";function g({components:e,...a}){return(0,r.yg)(m,(0,t.A)({},p,a,{components:e,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"reward-feedback-learning-reward-fl"},"Reward Feedback Learning (Reward FL)"),(0,r.yg)("h2",{id:"introduction"},"Introduction"),(0,r.yg)("p",null,"Reward Feedback Learning (Reward FL) is a reinforcement learning algorithm that optimize diffusion models against a scorer. Reward Fl works as follows:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Sampling"),": For a given prompt and first frame latent, the model generates a corresponding video."),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Reward Assignment"),": Each video is evaluated and assigned a reward based on its face informations."),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Model Update"),": The model updates its parameters based on reward signals from the generated videos, reinforcing strategies that obtain higher rewards.")),(0,r.yg)("h2",{id:"reward-fl-configuration-parameters"},"Reward FL Configuration Parameters"),(0,r.yg)("p",null,"In ROLL, the Reward FL algorithm-specific configuration parameters are as follows (",(0,r.yg)("inlineCode",{parentName:"p"},"roll.pipeline.diffusion.reward_fl.reward_fl_config.RewardFLConfig"),"):"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'# reward fl\nlearning_rate: 2.5e-6\nlr_scheduler_type: constant\nper_device_train_batch_size: 1\ngradient_accumulation_steps: 1\nwarmup_steps: 10\nnum_train_epochs: 1\n\nmodel_name: "wan2_2"\n\n# wan2_2 related\nmodel_paths: ./examples/wan2.2-14B-reward_fl_ds/wan22_paths.json\nreward_model_path: /data/models/antelopev2/\ntokenizer_path: /data/models/Wan-AI/Wan2.1-T2V-1.3B/google/umt5-xxl/\nmodel_id_with_origin_paths: null\ntrainable_models: dit2\nuse_gradient_checkpointing_offload: true\nextra_inputs: input_image\nmax_timestep_boundary: 1.0\nmin_timestep_boundary: 0.9\nnum_inference_steps: 8\n')),(0,r.yg)("h3",{id:"core-parameter-descriptions"},"Core Parameter Descriptions"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"num_train_epochs"),": Number of optimization rounds per batch of samples"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"train_batch_size"),": Batch size for one train step.In deepspeed training the global train batch size is ",(0,r.yg)("inlineCode",{parentName:"li"},"per_device_train_batch_size")," ","*"," ",(0,r.yg)("inlineCode",{parentName:"li"},"gradient_accumulation_steps")," ","*"," world_size"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"learning_rate"),": Learning rate"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"per_device_train_batch_size"),": Training batch size per device"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"gradient_accumulation_steps"),": Gradient accumulation steps"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"weight_decay"),": Weight decay coefficient"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"warmup_steps"),": Learning rate warmup steps"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"lr_scheduler_type"),": Learning rate scheduler type")),(0,r.yg)("h3",{id:"wan2_2-related-parameters"},"Wan2_2 Related Parameters"),(0,r.yg)("p",null,"The following parameters related to Wan2_2 are as follows:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"model_paths"),": Model path of json file, e.g., ",(0,r.yg)("inlineCode",{parentName:"li"},"wan22_paths.json"),", including high_noise_model, low_noise_model, text_encoder, vae."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"tokenizer_path"),": Tokenizer path. Leave empty to auto-download."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"reward_model_path"),": Reward model path, e.g., face model."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"max_timestep_boundary"),": Maximum value of the timestep interval, ranging from 0 to 1. Default is 1. This needs to be manually set only when training mixed models with multiple DiTs, for example, ",(0,r.yg)("a",{parentName:"li",href:"https://modelscope.cn/models/Wan-AI/Wan2.2-I2V-A14B"},"Wan-AI/Wan2.2-I2V-A14B"),"."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"min_timestep_boundary"),": Minimum value of the timestep interval, ranging from 0 to 1. Default is 1. This needs to be manually set only when training mixed models with multiple DiTs, for example, ",(0,r.yg)("a",{parentName:"li",href:"https://modelscope.cn/models/Wan-AI/Wan2.2-I2V-A14B"},"Wan-AI/Wan2.2-I2V-A14B"),"."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"model_id_with_origin_paths"),": Model ID with origin paths, e.g., Wan-AI/Wan2.1-T2V-1.3B:diffusion_pytorch_model*.safetensors. Comma-separated."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"trainable_models"),": Models to train, e.g., dit, vae, text_encoder."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"extra_inputs"),": Additional model inputs, comma-separated."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"use_gradient_checkpointing_offload"),": Whether to offload gradient checkpointing to CPU memory."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"num_inference_steps"),": Number of inference steps, default is 8 for the distilled wan2_2 model.")),(0,r.yg)("h2",{id:"note"},"Note"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"The reward model is constructed based on facial information, Please ensure that the first frame of the video contains a human face."),(0,r.yg)("li",{parentName:"ul"},"Download the reward model(antelopev2.zip) and unzip the onnx files to ",(0,r.yg)("inlineCode",{parentName:"li"},"reward_model_path")," directory."),(0,r.yg)("li",{parentName:"ul"},"Download the official Wan2.2 pipeline and Distilled Wan2.2 DiT safetensors. Put them in the ",(0,r.yg)("inlineCode",{parentName:"li"},"model_paths")," directory, e.g., ",(0,r.yg)("inlineCode",{parentName:"li"},"wan22_paths.json")," file."),(0,r.yg)("li",{parentName:"ul"},"According to the data/example_video_dataset/metadata.csv file, adapt your video dataset to the corresponding format")),(0,r.yg)("h2",{id:"refernece-model"},"Refernece Model"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"Official Wan2.2 pipeline"),": ",(0,r.yg)("a",{parentName:"li",href:"https://modelscope.cn/models/Wan-AI/Wan2.2-I2V-A14B"},"Wan-AI/Wan2.2-I2V-A14B")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"Distilled Wan2.2 DiT safetensors"),": ",(0,r.yg)("a",{parentName:"li",href:"https://huggingface.co/lightx2v/Wan2.2-Lightning/tree/main"},"lightx2v/Wan2.2-Lightning")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"Reward Model"),": ",(0,r.yg)("a",{parentName:"li",href:"https://github.com/deepinsight/insightface/releases/download/v0.7/antelopev2.zip"},"deepinsight/insightface")," ")),(0,r.yg)("h2",{id:"reference-example"},"Reference Example"),(0,r.yg)("p",null,"You can refer to the following configuration file to set up Reward FL training:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"./examples/docs_examples/example_reward_fl.yaml"))))}g.isMDXComponent=!0},5680:(e,a,n)=>{n.d(a,{xA:()=>p,yg:()=>u});var t=n(6540);function r(e,a,n){return a in e?Object.defineProperty(e,a,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[a]=n,e}function i(e,a){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter(function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable})),n.push.apply(n,t)}return n}function l(e){for(var a=1;a<arguments.length;a++){var n=null!=arguments[a]?arguments[a]:{};a%2?i(Object(n),!0).forEach(function(a){r(e,a,n[a])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach(function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(n,a))})}return e}function o(e,a){if(null==e)return{};var n,t,r=function(e,a){if(null==e)return{};var n,t,r={},i=Object.keys(e);for(t=0;t<i.length;t++)n=i[t],a.indexOf(n)>=0||(r[n]=e[n]);return r}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)n=i[t],a.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var d=t.createContext({}),s=function(e){var a=t.useContext(d),n=a;return e&&(n="function"==typeof e?e(a):l(l({},a),e)),n},p=function(e){var a=s(e.components);return t.createElement(d.Provider,{value:a},e.children)},m="mdxType",g={inlineCode:"code",wrapper:function(e){var a=e.children;return t.createElement(t.Fragment,{},a)}},c=t.forwardRef(function(e,a){var n=e.components,r=e.mdxType,i=e.originalType,d=e.parentName,p=o(e,["components","mdxType","originalType","parentName"]),m=s(n),c=r,u=m["".concat(d,".").concat(c)]||m[c]||g[c]||i;return n?t.createElement(u,l(l({ref:a},p),{},{components:n})):t.createElement(u,l({ref:a},p))});function u(e,a){var n=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var i=n.length,l=new Array(i);l[0]=c;var o={};for(var d in a)hasOwnProperty.call(a,d)&&(o[d]=a[d]);o.originalType=e,o[m]="string"==typeof e?e:r,l[1]=o;for(var s=2;s<i;s++)l[s]=n[s];return t.createElement.apply(null,l)}return t.createElement.apply(null,n)}c.displayName="MDXCreateElement"}}]);