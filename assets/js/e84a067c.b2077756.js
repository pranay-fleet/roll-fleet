"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[6585],{5680:(e,a,n)=>{n.d(a,{xA:()=>u,yg:()=>c});var t=n(6540);function i(e,a,n){return a in e?Object.defineProperty(e,a,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[a]=n,e}function r(e,a){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);a&&(t=t.filter(function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable})),n.push.apply(n,t)}return n}function l(e){for(var a=1;a<arguments.length;a++){var n=null!=arguments[a]?arguments[a]:{};a%2?r(Object(n),!0).forEach(function(a){i(e,a,n[a])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach(function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(n,a))})}return e}function o(e,a){if(null==e)return{};var n,t,i=function(e,a){if(null==e)return{};var n,t,i={},r=Object.keys(e);for(t=0;t<r.length;t++)n=r[t],a.indexOf(n)>=0||(i[n]=e[n]);return i}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(t=0;t<r.length;t++)n=r[t],a.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var s=t.createContext({}),p=function(e){var a=t.useContext(s),n=a;return e&&(n="function"==typeof e?e(a):l(l({},a),e)),n},u=function(e){var a=p(e.components);return t.createElement(s.Provider,{value:a},e.children)},g="mdxType",m={inlineCode:"code",wrapper:function(e){var a=e.children;return t.createElement(t.Fragment,{},a)}},d=t.forwardRef(function(e,a){var n=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),g=p(n),d=i,c=g["".concat(s,".").concat(d)]||g[d]||m[d]||r;return n?t.createElement(c,l(l({ref:a},u),{},{components:n})):t.createElement(c,l({ref:a},u))});function c(e,a){var n=arguments,i=a&&a.mdxType;if("string"==typeof e||i){var r=n.length,l=new Array(r);l[0]=d;var o={};for(var s in a)hasOwnProperty.call(a,s)&&(o[s]=a[s]);o.originalType=e,o[g]="string"==typeof e?e:i,l[1]=o;for(var p=2;p<r;p++)l[p]=n[p];return t.createElement.apply(null,l)}return t.createElement.apply(null,n)}d.displayName="MDXCreateElement"},9407:(e,a,n)=>{n.r(a),n.d(a,{assets:()=>s,contentTitle:()=>l,default:()=>m,frontMatter:()=>r,metadata:()=>o,toc:()=>p});var t=n(8168),i=(n(6540),n(5680));const r={},l="Lite PPO",o={unversionedId:"English/UserGuide/algorithms/LitePPO",id:"English/UserGuide/algorithms/LitePPO",title:"Lite PPO",description:"Introduction",source:"@site/docs/English/UserGuide/algorithms/LitePPO.md",sourceDirName:"English/UserGuide/algorithms",slug:"/English/UserGuide/algorithms/LitePPO",permalink:"/ROLL/docs/English/UserGuide/algorithms/LitePPO",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/algorithms/LitePPO.md",tags:[],version:"current",lastUpdatedAt:1761386293,formattedLastUpdatedAt:"Oct 25, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Group Sequence Policy Optimization (GSPO)",permalink:"/ROLL/docs/English/UserGuide/algorithms/GSPO"},next:{title:"Proximal Policy Optimization (PPO)",permalink:"/ROLL/docs/English/UserGuide/algorithms/PPO"}},s={},p=[{value:"Introduction",id:"introduction",level:2},{value:"LitePPO Configuration Parameters",id:"liteppo-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"PPO Related Parameters",id:"ppo-related-parameters",level:3},{value:"References",id:"references",level:2}],u={toc:p},g="wrapper";function m({components:e,...a}){return(0,i.yg)(g,(0,t.A)({},u,a,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"lite-ppo"},"Lite PPO"),(0,i.yg)("h2",{id:"introduction"},"Introduction"),(0,i.yg)("p",null,'LitePPO is a lightweight proximal policy optimization algorithm designed for efficient training of large language models. LitePPO improves training efficiency and stability through token-level loss computation and "within-group mean + batch standard deviation normalization" only. LitePPO works as follows:'),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Token-level Loss Computation"),": Computes losses at the token level to improve training granularity and efficiency."),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Group-level Reward Normalization"),': Uses "within-group mean + batch standard deviation normalization" to stabilize the training process.'),(0,i.yg)("li",{parentName:"ol"},(0,i.yg)("strong",{parentName:"li"},"Redundancy Removal Design"),": Removes unnecessary components such as overlong filtering, preserving the original PPO objective function.")),(0,i.yg)("h2",{id:"liteppo-configuration-parameters"},"LitePPO Configuration Parameters"),(0,i.yg)("p",null,"In ROLL, the LitePPO algorithm-specific configuration parameters are as follows (",(0,i.yg)("inlineCode",{parentName:"p"},"roll.pipeline.rlvr.rlvr_config.RLVRConfig"),"):"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},'# LitePPO core config\n## normalization\nnorm_mean_type: group\nnorm_std_type: batch\n\n## token-level loss \ntoken_level_loss: true\n\n# ppo related, other parts are compatible with GRPO/PPO settings\nrollout_batch_size: 512  # prompt\nprompt_length: 2048\nresponse_length: 4096\n\nadv_estimator: "gae"\nnum_return_sequences_in_group: 1\nppo_epochs: 1\nuse_kl_loss: true\nkl_loss_coef: 0.001\nloss_agg_mode: "seq-mean-token-mean"\n\n\nwhiten_advantages: true\nadvantage_clip: 2.0\nreward_clip: ~\ndual_clip_loss: true\nlambd: 0.95\ngamma: 1\npg_clip: 0.2\nvalue_clip: ~\nkl_penalty: "kl"\ntarget_kl: ~\ninit_kl_coef: 0.2\nkl_horizon: 10000\nadd_token_level_kl: false\n# normalize\nreward_shift: false\nreward_scale: false\n')),(0,i.yg)("h3",{id:"core-parameter-descriptions"},"Core Parameter Descriptions"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"norm_mean_type"),': Mean type for reward normalization: the options are "batch", "group", "running", or None; the default is None'),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"norm_std_type"),': Std type for reward normalization: the options are "batch", "group", "running", or None; the default is None'),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"token_level_loss"),": Whether to enable token-level loss computation, default value is true")),(0,i.yg)("h3",{id:"ppo-related-parameters"},"PPO Related Parameters"),(0,i.yg)("p",null,"The following parameters are common configuration items for PPO-class algorithms:"),(0,i.yg)("ul",null,(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"rollout_batch_size"),": Number of prompts per rollout_batch_size, default value is 512"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"prompt_length"),": Maximum length of prompts, default value is 2048"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"response_length"),": Maximum length of responses, default value is 4096"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"adv_estimator"),': Advantage estimator type, optional values are "gae", "reinforce", "grpo", default value is "gae"'),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"num_return_sequences_in_group"),": Number of responses generated per prompt (group size), default value is 1"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"ppo_epochs"),": Number of optimization rounds per batch of samples, default value is 1"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"use_kl_loss"),": Whether to use KL divergence loss, default value is true"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"kl_loss_coef"),": KL-loss coefficient, default value is 0.001"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"loss_agg_mode"),': Loss aggregation mode, default is "seq-mean-token-sum", optional values are "token-mean", "seq-mean-token-sum", "seq-mean-token-mean", "seq-mean-token-sum-norm"'),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"whiten_advantages"),": Whether to whiten advantage values, default value is true"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"advantage_clip"),": Advantage value clipping range, default value is 2.0"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"reward_clip"),": Reward value clipping range, default value is ~ (means not set)"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"dual_clip_loss"),": Whether to use dual clipping loss, default value is true"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"lambd"),": Lambda parameter in GAE estimator, used to trade off bias and variance, default value is 0.95"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"gamma"),": Discount factor, default value is 1"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"pg_clip"),": PPO clipping range, default value is 0.2"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"value_clip"),": Value function clipping range, default value is ~ (means not set)"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"kl_penalty"),': KL penalty options, optional values are "kl", "abs", "mse", "full", default value is "kl"'),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"target_kl"),": Target KL value for adaptive KL control, default value is ~ (means not set)"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"init_kl_coef"),": Initial KL penalty coefficient, default value is 0.2"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"kl_horizon"),": Range for adaptive KL control, default value is 10000"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"add_token_level_kl"),": Whether to add token-level KL penalty, default value is false"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"reward_shift"),": Whether to only subtract mean in reward normalization, default value is false"),(0,i.yg)("li",{parentName:"ul"},(0,i.yg)("inlineCode",{parentName:"li"},"reward_scale"),": Whether to only divide by standard deviation in reward normalization, default value is false")),(0,i.yg)("h2",{id:"references"},"References"),(0,i.yg)("p",null,"[1]"," Liu, Z.; Liu, J.; He, Y.; Wang, W.; Liu, J.; Pan, L.; Hu, X.; Xiong, S.; Huang, J.; Hu, J.; Huang, S.; Yang, S.; Wang, J.; Su, W.; Zheng, B. Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning. arXiv August 11, 2025. ",(0,i.yg)("a",{parentName:"p",href:"https://doi.org/10.48550/arXiv.2508.08221"},"https://doi.org/10.48550/arXiv.2508.08221"),"."))}m.isMDXComponent=!0}}]);