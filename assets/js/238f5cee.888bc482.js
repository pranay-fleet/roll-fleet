"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[9070],{4680:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>u,frontMatter:()=>t,metadata:()=>l,toc:()=>p});var a=r(8168),i=(r(6540),r(5680));const t={},o="ROLL Resource Configuration",l={unversionedId:"English/UserGuide/device_mapping",id:"English/UserGuide/device_mapping",title:"ROLL Resource Configuration",description:"In the ROLL framework, resource settings are specified through the device_mapping parameter in YAML configuration files to determine which GPU devices each worker uses. This document will provide detailed instructions on how to configure resources, including colocated and disaggregated modes, multi-role resource configuration, and how worker counts are calculated.",source:"@site/docs/English/UserGuide/device_mapping.md",sourceDirName:"English/UserGuide",slug:"/English/UserGuide/device_mapping",permalink:"/ROLL/docs/English/UserGuide/device_mapping",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/device_mapping.md",tags:[],version:"current",lastUpdatedAt:1761384892,formattedLastUpdatedAt:"Oct 25, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Checkpoint Saving and Resuming Guide",permalink:"/ROLL/docs/English/UserGuide/checkpoint_and_resume"},next:{title:"Converting MCoreAdapter Models to Hugging Face Format",permalink:"/ROLL/docs/English/UserGuide/megatron_convert_2_hf"}},s={},p=[{value:"GPU Resource Configuration",id:"gpu-resource-configuration",level:2},{value:"CPU Resource Configuration",id:"cpu-resource-configuration",level:2},{value:"Colocated and Disaggregated Modes",id:"colocated-and-disaggregated-modes",level:2},{value:"Colocated Mode",id:"colocated-mode",level:3},{value:"Disaggregated Mode",id:"disaggregated-mode",level:3},{value:"Flexibility in Multi-Role Resource Configuration",id:"flexibility-in-multi-role-resource-configuration",level:2},{value:"Worker Count Calculation",id:"worker-count-calculation",level:2},{value:"Configuration Example",id:"configuration-example",level:2}],c={toc:p},g="wrapper";function u({components:e,...n}){return(0,i.yg)(g,(0,a.A)({},c,n,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"roll-resource-configuration"},"ROLL Resource Configuration"),(0,i.yg)("p",null,"In the ROLL framework, resource settings are specified through the ",(0,i.yg)("inlineCode",{parentName:"p"},"device_mapping")," parameter in YAML configuration files to determine which GPU devices each worker uses. This document will provide detailed instructions on how to configure resources, including colocated and disaggregated modes, multi-role resource configuration, and how worker counts are calculated."),(0,i.yg)("h2",{id:"gpu-resource-configuration"},"GPU Resource Configuration"),(0,i.yg)("p",null,"In ROLL, GPU resource settings are configured by specifying the ",(0,i.yg)("inlineCode",{parentName:"p"},"device_mapping")," parameter for each worker in the YAML configuration file. This parameter is a string that can be parsed by Python's ",(0,i.yg)("inlineCode",{parentName:"p"},"eval()")," function into a list, where the values in the list represent global logical GPU RANKs."),(0,i.yg)("p",null,"For example:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},"actor_train:\n  device_mapping: list(range(0,16))\nactor_infer:\n  device_mapping: list(range(16,24))\n")),(0,i.yg)("p",null,"In this example, the system requires a total of 24 GPUs, where ",(0,i.yg)("inlineCode",{parentName:"p"},"actor_train")," is deployed on GPUs [0,16) and ",(0,i.yg)("inlineCode",{parentName:"p"},"actor_infer")," is deployed on GPUs [16,24)."),(0,i.yg)("h2",{id:"cpu-resource-configuration"},"CPU Resource Configuration"),(0,i.yg)("p",null,"For workers that only use CPU resources, simply configure the ",(0,i.yg)("inlineCode",{parentName:"p"},"world_size")," parameter, and the system will automatically deploy the corresponding number of workers (ray.Actor) on CPU resources."),(0,i.yg)("p",null,"For example:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},"code_sandbox:\n  world_size: 8\n")),(0,i.yg)("h2",{id:"colocated-and-disaggregated-modes"},"Colocated and Disaggregated Modes"),(0,i.yg)("h3",{id:"colocated-mode"},"Colocated Mode"),(0,i.yg)("p",null,"In colocated mode, multiple roles share the same GPU resources. This approach can improve resource utilization and reduce resource waste."),(0,i.yg)("p",null,"For example, in ",(0,i.yg)("inlineCode",{parentName:"p"},"examples/docs_examples/example_grpo.yaml"),":"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},"actor_infer:\n  device_mapping: list(range(0,8)) # Shares GPUs [0,8) with actor_train, GPU time-division multiplexing\n# ...\nactor_train:\n  device_mapping: list(range(0,8))\n")),(0,i.yg)("h3",{id:"disaggregated-mode"},"Disaggregated Mode"),(0,i.yg)("p",null,"In disaggregated mode, different roles use different GPU resources. This independent deployment approach is key to implementing asynchronous training.\nROLL directly implements disaggregated deployment by setting different ",(0,i.yg)("inlineCode",{parentName:"p"},"device_mapping")," for different workers."),(0,i.yg)("p",null,"For example, in ",(0,i.yg)("inlineCode",{parentName:"p"},"examples/qwen2.5-7B-agentic_megatron/agentic_val_webshop_async.yaml"),":"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},"# actor train uses GPUs [0, 1, 2, 3], actor_infer uses GPUs [4, 5, 6, 7]\n# \nactor_train:\n  device_mapping: list(range(0,4))\nactor_infer:\n  device_mapping: list(range(4,8))\n")),(0,i.yg)("h2",{id:"flexibility-in-multi-role-resource-configuration"},"Flexibility in Multi-Role Resource Configuration"),(0,i.yg)("p",null,"The ROLL framework supports configuring different resource strategies for different roles to meet the needs of various application scenarios:"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"Different roles can use different numbers of GPUs"),(0,i.yg)("li",{parentName:"ol"},"Different roles can use different inference engines (such as vLLM, SGLang, etc.)"),(0,i.yg)("li",{parentName:"ol"},"Different roles can set different ",(0,i.yg)("inlineCode",{parentName:"li"},"num_gpus_per_worker")," parameters",(0,i.yg)("ol",{parentName:"li"},(0,i.yg)("li",{parentName:"ol"},"The num_gpus_per_worker for training roles is always 1"),(0,i.yg)("li",{parentName:"ol"},"The num_gpus_per_worker for inference roles is >=1 based on required resources, with vLLM automatically calculated based on parallel settings and SGLang directly specified")))),(0,i.yg)("p",null,"For example, when using vLLM as the inference engine, ",(0,i.yg)("inlineCode",{parentName:"p"},"num_gpus_per_worker")," is automatically calculated based on tensor parallel (tensor_parallel_size) and pipeline parallel (pipeline_parallel_size) settings:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},"actor_infer:\n  strategy_args:\n    strategy_name: vllm\n    strategy_config:\n      tensor_parallel_size: 2\n      pipeline_parallel_size: 1\n  num_gpus_per_worker: 2  # Automatically calculated as tensor_parallel_size * pipeline_parallel_size\n")),(0,i.yg)("h2",{id:"worker-count-calculation"},"Worker Count Calculation"),(0,i.yg)("p",null,"The number of workers (",(0,i.yg)("inlineCode",{parentName:"p"},"world_size"),") is automatically calculated based on the ",(0,i.yg)("inlineCode",{parentName:"p"},"device_mapping")," and ",(0,i.yg)("inlineCode",{parentName:"p"},"num_gpus_per_worker")," parameters:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-python"},"world_size = len(device_mapping) // num_gpus_per_worker\n")),(0,i.yg)("p",null,"In the ",(0,i.yg)("inlineCode",{parentName:"p"},"WorkerConfig.__post_init__()")," method, if ",(0,i.yg)("inlineCode",{parentName:"p"},"device_mapping")," is not None, the following logic is executed:"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"Parse the string into a list through ",(0,i.yg)("inlineCode",{parentName:"li"},"eval(device_mapping)")),(0,i.yg)("li",{parentName:"ol"},"Verify that ",(0,i.yg)("inlineCode",{parentName:"li"},"len(device_mapping)")," is divisible by ",(0,i.yg)("inlineCode",{parentName:"li"},"num_gpus_per_worker")),(0,i.yg)("li",{parentName:"ol"},"Calculate ",(0,i.yg)("inlineCode",{parentName:"li"},"world_size = len(device_mapping) // num_gpus_per_worker"))),(0,i.yg)("p",null,"For workers that only use CPU, the worker count is directly specified through the ",(0,i.yg)("inlineCode",{parentName:"p"},"world_size")," parameter, and ",(0,i.yg)("inlineCode",{parentName:"p"},"num_gpus_per_worker")," is set to 0."),(0,i.yg)("h2",{id:"configuration-example"},"Configuration Example"),(0,i.yg)("p",null,"The following is a complete resource configuration example:"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},"num_gpus_per_node: 8\n\nactor_train:\n  device_mapping: list(range(0,16))  # Uses 16 GPUs\n  # world_size automatically calculated as 16 // 1 = 16\n\nactor_infer:\n  num_gpus_per_worker: 2  # Each worker uses 2 GPUs\n  device_mapping: list(range(0,12))  # Uses 12 GPUs\n  # world_size automatically calculated as 12 // 2 = 6\n\nrewards:\n  code_sandbox:\n    world_size: 8  # CPU-only, deploys 8 workers\n")),(0,i.yg)("p",null,"By properly configuring these parameters, resources can be flexibly allocated to different roles to meet various training and inference requirements."))}u.isMDXComponent=!0},5680:(e,n,r)=>{r.d(n,{xA:()=>c,yg:()=>m});var a=r(6540);function i(e,n,r){return n in e?Object.defineProperty(e,n,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[n]=r,e}function t(e,n){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),r.push.apply(r,a)}return r}function o(e){for(var n=1;n<arguments.length;n++){var r=null!=arguments[n]?arguments[n]:{};n%2?t(Object(r),!0).forEach(function(n){i(e,n,r[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):t(Object(r)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(r,n))})}return e}function l(e,n){if(null==e)return{};var r,a,i=function(e,n){if(null==e)return{};var r,a,i={},t=Object.keys(e);for(a=0;a<t.length;a++)r=t[a],n.indexOf(r)>=0||(i[r]=e[r]);return i}(e,n);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);for(a=0;a<t.length;a++)r=t[a],n.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(i[r]=e[r])}return i}var s=a.createContext({}),p=function(e){var n=a.useContext(s),r=n;return e&&(r="function"==typeof e?e(n):o(o({},n),e)),r},c=function(e){var n=p(e.components);return a.createElement(s.Provider,{value:n},e.children)},g="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},d=a.forwardRef(function(e,n){var r=e.components,i=e.mdxType,t=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),g=p(r),d=i,m=g["".concat(s,".").concat(d)]||g[d]||u[d]||t;return r?a.createElement(m,o(o({ref:n},c),{},{components:r})):a.createElement(m,o({ref:n},c))});function m(e,n){var r=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var t=r.length,o=new Array(t);o[0]=d;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[g]="string"==typeof e?e:i,o[1]=l;for(var p=2;p<t;p++)o[p]=r[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,r)}d.displayName="MDXCreateElement"}}]);