"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[8807],{4847:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>p,toc:()=>g});var a=i(8168),t=(i(6540),i(5680));const r={},l="Agentic Pipeline",p={unversionedId:"English/UserGuide/pipeline/agentic_pipeline_start",id:"English/UserGuide/pipeline/agentic_pipeline_start",title:"Agentic Pipeline",description:"Table of Contents",source:"@site/docs/English/UserGuide/pipeline/agentic_pipeline_start.md",sourceDirName:"English/UserGuide/pipeline",slug:"/English/UserGuide/pipeline/agentic_pipeline_start",permalink:"/ROLL/docs/English/UserGuide/pipeline/agentic_pipeline_start",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/pipeline/agentic_pipeline_start.md",tags:[],version:"current",lastUpdatedAt:1761384892,formattedLastUpdatedAt:"Oct 25, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Comprehensive Guide: Using the Agentic Part of ROLL",permalink:"/ROLL/docs/English/UserGuide/pipeline/agent_pipeline_start"},next:{title:"Distill Pipeline",permalink:"/ROLL/docs/English/UserGuide/pipeline/distill_pipeline_start"}},o={},g=[{value:"\u2728\ufe0f Overview",id:"\ufe0f-overview",level:2},{value:"\u2728\ufe0f Core Components",id:"\ufe0f-core-components",level:2},{value:"Main Module (<code>AgenticPipeline</code>)",id:"main-module-agenticpipeline",level:3},{value:"Configuration File (<code>AgenticConfig</code>)",id:"configuration-file-agenticconfig",level:3},{value:"Configuration Structure and Organization",id:"configuration-structure-and-organization",level:4},{value:"\u2728\ufe0f Environment Preparation",id:"\ufe0f-environment-preparation",level:2},{value:"Environment Types",id:"environment-types",level:3},{value:"Environment Configuration",id:"environment-configuration",level:3},{value:"\u2728\ufe0f Running the Pipeline",id:"\ufe0f-running-the-pipeline",level:2},{value:"Method 1: Using Python Startup Script",id:"method-1-using-python-startup-script",level:3},{value:"Method 2: Using Helper Shell Script",id:"method-2-using-helper-shell-script",level:3},{value:"\u2728\ufe0f Step-by-Step Example",id:"\ufe0f-step-by-step-example",level:2},{value:"Step 1: Configuration Setup",id:"step-1-configuration-setup",level:3},{value:"Step 2: Environment and Dependency Preparation",id:"step-2-environment-and-dependency-preparation",level:3},{value:"Step 3: Starting the Pipeline",id:"step-3-starting-the-pipeline",level:3},{value:"Step 4: Monitoring",id:"step-4-monitoring",level:3},{value:"Step 5: Output and Results",id:"step-5-output-and-results",level:3}],m={toc:g},s="wrapper";function u({components:e,...n}){return(0,t.yg)(s,(0,a.A)({},m,n,{components:e,mdxType:"MDXLayout"}),(0,t.yg)("h1",{id:"agentic-pipeline"},"Agentic Pipeline"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Table of Contents")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#agentic-pipeline"},"Agentic Pipeline"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#%EF%B8%8F-overview"},"\u2728\ufe0f Overview")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#%EF%B8%8F-core-components"},"\u2728\ufe0f Core Components"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#main-module-agenticpipeline"},"Main Module (",(0,t.yg)("inlineCode",{parentName:"a"},"AgenticPipeline"),")")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#configuration-file-agenticconfig"},"Configuration File (",(0,t.yg)("inlineCode",{parentName:"a"},"AgenticConfig"),")"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#configuration-structure-and-organization"},"Configuration Structure and Organization")))))),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#%EF%B8%8F-environment-preparation"},"\u2728\ufe0f Environment Preparation"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#environment-types"},"Environment Types")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#environment-configuration"},"Environment Configuration")))),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#%EF%B8%8F-running-the-pipeline"},"\u2728\ufe0f Running the Pipeline"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#method-1-using-python-startup-script"},"Method 1: Using Python Startup Script")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#method-2-using-helper-shell-script"},"Method 2: Using Helper Shell Script")))),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#%EF%B8%8F-step-by-step-example"},"\u2728\ufe0f Step-by-Step Example"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#step-1-configuration-setup"},"Step 1: Configuration Setup")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#step-2-environment-and-dependency-preparation"},"Step 2: Environment and Dependency Preparation")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#step-3-starting-the-pipeline"},"Step 3: Starting the Pipeline")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#step-4-monitoring"},"Step 4: Monitoring")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#step-5-output-and-results"},"Step 5: Output and Results"))))))),(0,t.yg)("hr",null),(0,t.yg)("h2",{id:"\ufe0f-overview"},"\u2728\ufe0f Overview"),(0,t.yg)("p",null,"Agentic Pipeline is ROLL's core pipeline for agent training, supporting multiple algorithms such as PPO, GRPO, and more. It provides the following core advantages:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Gym-like Environment Definition"),": Supports various environment types, including FrozenLake, Sokoban, etc., and can easily extend custom environments through gym-like interfaces."),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Rich Learning Granularity"),": Supports TrajectoryWise form (StarPO) and StepWise (GiGPO) training forms."),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Asynchronous Parallel Rollout at Environment Granularity"),": Independent trajectory sampling across environments improves sampling efficiency."),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Asynchronous Training"),": Decoupling of rollout/training supports asynchronous training."),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Multi-turn Interaction Support for Local Debugging"),": Multi-turn interaction rollout supports local debugging, improving development efficiency for multi-turn interaction business."),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Flexible Policy Configuration"),": Supports multiple distributed training strategies such as Megatron, DeepSpeed, vLLM, etc., allowing flexible configuration based on hardware resources.")),(0,t.yg)("hr",null),(0,t.yg)("h2",{id:"\ufe0f-core-components"},"\u2728\ufe0f Core Components"),(0,t.yg)("h3",{id:"main-module-agenticpipeline"},"Main Module (",(0,t.yg)("inlineCode",{parentName:"h3"},"AgenticPipeline"),")"),(0,t.yg)("p",null,(0,t.yg)("inlineCode",{parentName:"p"},"AgenticPipeline")," (located at ",(0,t.yg)("inlineCode",{parentName:"p"},"roll/pipeline/agentic/agentic_pipeline.py"),") is the main process for the entire agent training. It manages the complete training workflow, including:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Initializing and managing distributed worker processes (Actor, Critic, Reference, etc.)."),(0,t.yg)("li",{parentName:"ul"},"Coordinating environment interaction and data collection."),(0,t.yg)("li",{parentName:"ul"},"Executing model training steps."),(0,t.yg)("li",{parentName:"ul"},"Handling checkpoint saving."),(0,t.yg)("li",{parentName:"ul"},"Recording metrics and experiment tracking.")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Source Code"),": ",(0,t.yg)("inlineCode",{parentName:"p"},"roll/pipeline/agentic/agentic_pipeline.py")),(0,t.yg)("hr",null),(0,t.yg)("h3",{id:"configuration-file-agenticconfig"},"Configuration File (",(0,t.yg)("inlineCode",{parentName:"h3"},"AgenticConfig"),")"),(0,t.yg)("p",null,(0,t.yg)("inlineCode",{parentName:"p"},"AgenticConfig")," (defined in ",(0,t.yg)("inlineCode",{parentName:"p"},"roll/pipeline/agentic/agentic_config.py"),") is a configuration object based on Pydantic/dataclass used to specify all parameters for running AgenticPipeline. This configuration system supports YAML file configuration and uses the Hydra framework for management."),(0,t.yg)("p",null,"For configuration system description, see ",(0,t.yg)("a",{parentName:"p",href:"/ROLL/docs/English/QuickStart/config_system"},"config_system")),(0,t.yg)("h4",{id:"configuration-structure-and-organization"},"Configuration Structure and Organization"),(0,t.yg)("p",null,"Configuration files (such as ",(0,t.yg)("inlineCode",{parentName:"p"},"examples/qwen2.5-0.5B-agentic/agent_val_frozen_lake.yaml"),") are organized by functional modules and mainly include the following sections:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Basic Experiment Settings")),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"exp_name"),": Experiment name, used to identify a specific training task"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"seed"),": Random seed to ensure reproducible experiments"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"logging_dir"),": Path to save log files"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"output_dir"),": Path to save model checkpoints and output files"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"render_save_dir"),": Path to save rendered frames (for environment visualization)"))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Training Control Parameters")),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"max_steps"),": Maximum training steps"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"save_steps"),": Frequency of saving model checkpoints"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"logging_steps"),": Frequency of recording training metrics"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"eval_steps"),": Frequency of performing validation evaluation"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"resume_from_checkpoint"),": Whether to resume training from a checkpoint. To continue training, set to its path; otherwise, set to ",(0,t.yg)("inlineCode",{parentName:"li"},"False"),"."))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Model Configuration")),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"pretrain"),": Pretrained model path"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"reward_pretrain"),": Reward model pretrained weights path"))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Algorithm Parameters")),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"adv_estimator"),": Advantage estimator type (such as ",(0,t.yg)("inlineCode",{parentName:"li"},"gae"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"grpo"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"reinforce"),")"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"ppo_epochs"),": Number of optimization epochs per sample batch"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"gamma"),": Discount factor for calculating returns"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"lambd"),": Lambda parameter in GAE"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"pg_clip"),": Clipping range for PPO policy gradient loss"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"init_kl_coef"),": Initial coefficient for KL penalty"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"target_kl"),": Target KL value for adaptive KL control"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"whiten_advantages"),": Whether to whiten advantages"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"entropy_loss_coef"),": Coefficient for entropy loss"))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Worker Process Configuration"),"\nEach worker process (",(0,t.yg)("inlineCode",{parentName:"p"},"actor_train"),", ",(0,t.yg)("inlineCode",{parentName:"p"},"actor_infer"),", ",(0,t.yg)("inlineCode",{parentName:"p"},"critic"),", ",(0,t.yg)("inlineCode",{parentName:"p"},"reference"),") configuration includes:"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Model Parameters")," (",(0,t.yg)("inlineCode",{parentName:"li"},"model_args"),")",(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"model_type"),": Model type (such as ",(0,t.yg)("inlineCode",{parentName:"li"},"causal_lm"),")"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"dtype"),": Computation precision (such as ",(0,t.yg)("inlineCode",{parentName:"li"},"bf16"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"fp16"),")"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"attn_implementation"),": Attention implementation (such as ",(0,t.yg)("inlineCode",{parentName:"li"},"fa2"),")"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"disable_gradient_checkpointing"),": Whether to disable gradient checkpointing"))),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Training Parameters")," (",(0,t.yg)("inlineCode",{parentName:"li"},"training_args"),")",(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"learning_rate"),": Learning rate"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"per_device_train_batch_size"),": Training batch size per device"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"gradient_accumulation_steps"),": Gradient accumulation steps"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"weight_decay"),": Weight decay coefficient"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"warmup_steps"),": Learning rate warmup steps"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"lr_scheduler_type"),": Learning rate scheduler type"))),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Generation Parameters")," (",(0,t.yg)("inlineCode",{parentName:"li"},"generating_args"),")",(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"max_new_tokens"),": Maximum new tokens to generate"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"top_p"),": Nucleus sampling parameter"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"temperature"),": Temperature parameter"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"num_return_sequences"),": Number of return sequences"))),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Distributed Strategy")," (",(0,t.yg)("inlineCode",{parentName:"li"},"strategy_args"),")",(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"strategy_name"),": Distributed strategy used (such as ",(0,t.yg)("inlineCode",{parentName:"li"},"megatron_train"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"vllm"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"hf_infer"),")"),(0,t.yg)("li",{parentName:"ul"},"Strategy-specific parameters: such as ",(0,t.yg)("inlineCode",{parentName:"li"},"tp_size")," (tensor parallel size), ",(0,t.yg)("inlineCode",{parentName:"li"},"pp_size")," (pipeline parallel size)"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"gpu_memory_utilization"),": GPU memory utilization (specific to vLLM)"))),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Device Mapping")," (",(0,t.yg)("inlineCode",{parentName:"li"},"device_mapping"),")",(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},"Specifies which GPU devices the worker process should use"))))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Environment Manager Configuration")),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"train_env_manager"),": Training environment manager configuration"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"val_env_manager"),": Validation environment manager configuration"),(0,t.yg)("li",{parentName:"ul"},"Environment-related parameters:",(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"num_env_groups"),": Number of environment groups"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"group_size"),": Number of environments per group"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"tags"),": List of environment tags"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"num_groups_partition"),": Group allocation for each environment type"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"max_env_num_per_worker"),": Maximum number of environments per worker")))))),(0,t.yg)("hr",null),(0,t.yg)("h2",{id:"\ufe0f-environment-preparation"},"\u2728\ufe0f Environment Preparation"),(0,t.yg)("h3",{id:"environment-types"},"Environment Types"),(0,t.yg)("p",null,"Agentic Pipeline supports various environment types, including but not limited to:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"FrozenLake"),": Classic reinforcement learning environment where the agent needs to find a path to the goal on ice."),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Sokoban"),": Box-pushing game environment where the agent needs to push boxes to designated positions."),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"WebShop"),": Simulated online shopping environment where the agent needs to find suitable products based on user requirements."),(0,t.yg)("li",{parentName:"ul"},"More environment support...")),(0,t.yg)("h3",{id:"environment-configuration"},"Environment Configuration"),(0,t.yg)("p",null,"In the configuration file, custom environments are defined through the ",(0,t.yg)("inlineCode",{parentName:"p"},"custom_envs")," field. Each environment configuration includes:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"env_type"),": Environment type"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"env_config"),": Specific environment configuration parameters"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"max_tokens_per_step"),": Maximum tokens per step")),(0,t.yg)("hr",null),(0,t.yg)("h2",{id:"\ufe0f-running-the-pipeline"},"\u2728\ufe0f Running the Pipeline"),(0,t.yg)("h3",{id:"method-1-using-python-startup-script"},"Method 1: Using Python Startup Script"),(0,t.yg)("p",null,"The main method is to use the ",(0,t.yg)("inlineCode",{parentName:"p"},"examples/start_agentic_pipeline.py")," script. This script uses Hydra to load and manage configurations."),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Select or Create a Configuration File"),(0,t.yg)("br",{parentName:"p"}),"\n","Start with example YAML (such as ",(0,t.yg)("inlineCode",{parentName:"p"},"examples/qwen2.5-0.5B-agentic/agent_val_frozen_lake.yaml"),") or create your own configuration.")),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Execute the Python Startup Script")),(0,t.yg)("pre",{parentName:"li"},(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"# Make sure you are in the ROLL project root directory\n# export PYTHONPATH=$(pwd):$PYTHONPATH\n\npython examples/start_agentic_pipeline.py \\\n       --config_path examples/qwen2.5-0.5B-agentic \\\n       --config_name agent_val_frozen_lake\n")),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"--config_path")," \u2013 Directory containing the YAML configuration."),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"--config_name")," \u2013 File name (without ",(0,t.yg)("inlineCode",{parentName:"li"},".yaml"),").")))),(0,t.yg)("h3",{id:"method-2-using-helper-shell-script"},"Method 2: Using Helper Shell Script"),(0,t.yg)("p",null,"The ",(0,t.yg)("inlineCode",{parentName:"p"},"examples")," directory typically contains shell scripts that wrap the Python launcher."),(0,t.yg)("p",null,"Example structure:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"#!/bin/bash\n# Example: examples/qwen2.5-0.5B-agentic/run_agentic_pipeline_frozen_lake.sh\n\nCONFIG_PATH=$(basename $(dirname $0))\npython examples/start_agentic_pipeline.py \\\n       --config_path $CONFIG_PATH \\\n       --config_name agent_val_frozen_lake\n")),(0,t.yg)("p",null,"Running method:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"bash examples/qwen2.5-0.5B-agentic/run_agentic_pipeline_frozen_lake.sh\n")),(0,t.yg)("hr",null),(0,t.yg)("h2",{id:"\ufe0f-step-by-step-example"},"\u2728\ufe0f Step-by-Step Example"),(0,t.yg)("h3",{id:"step-1-configuration-setup"},"Step 1: Configuration Setup"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},"File: ",(0,t.yg)("inlineCode",{parentName:"p"},"examples/qwen2.5-0.5B-agentic/agent_val_frozen_lake.yaml"),(0,t.yg)("br",{parentName:"p"}),"\n","Key sections include ",(0,t.yg)("inlineCode",{parentName:"p"},"exp_name"),", ",(0,t.yg)("inlineCode",{parentName:"p"},"seed"),", ",(0,t.yg)("inlineCode",{parentName:"p"},"output_dir"),", model paths, and worker process configurations.")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},"Pay special attention to these configuration sections:"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},"Model configuration: ",(0,t.yg)("inlineCode",{parentName:"li"},"pretrain")," path"),(0,t.yg)("li",{parentName:"ul"},"Algorithm parameters: ",(0,t.yg)("inlineCode",{parentName:"li"},"adv_estimator"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"ppo_epochs"),", etc."),(0,t.yg)("li",{parentName:"ul"},"Distributed strategy: ",(0,t.yg)("inlineCode",{parentName:"li"},"strategy_args")," and ",(0,t.yg)("inlineCode",{parentName:"li"},"device_mapping")," for each worker process"),(0,t.yg)("li",{parentName:"ul"},"Environment configuration: ",(0,t.yg)("inlineCode",{parentName:"li"},"train_env_manager")," and ",(0,t.yg)("inlineCode",{parentName:"li"},"val_env_manager"))))),(0,t.yg)("h3",{id:"step-2-environment-and-dependency-preparation"},"Step 2: Environment and Dependency Preparation"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},"Ensure all necessary dependencies are installed, it's recommended to start from ",(0,t.yg)("a",{parentName:"p",href:"/ROLL/docs/English/QuickStart/installation"},"image launch"),":"),(0,t.yg)("pre",{parentName:"li"},(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"pip install -r requirements.txt\n"))),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},"Confirm all model paths in the configuration are accessible.")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},"Prepare the training environment and ensure support for the selected environment types."))),(0,t.yg)("h3",{id:"step-3-starting-the-pipeline"},"Step 3: Starting the Pipeline"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"python examples/start_agentic_pipeline.py \\\n       --config_path examples/qwen2.5-0.5B-agentic \\\n       --config_name agent_val_frozen_lake\n")),(0,t.yg)("h3",{id:"step-4-monitoring"},"Step 4: Monitoring"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Console Output")," \u2013 Observe Hydra, Ray, and Pipeline logs.")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Log Files")," \u2013 Check the ",(0,t.yg)("inlineCode",{parentName:"p"},"logging_dir")," specified in the YAML.")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"TensorBoard")),(0,t.yg)("pre",{parentName:"li"},(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"tensorboard --logdir <your_log_dir>\n")))),(0,t.yg)("h3",{id:"step-5-output-and-results"},"Step 5: Output and Results"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Trained Model")," \u2013 Checkpoints are saved in ",(0,t.yg)("inlineCode",{parentName:"li"},"checkpoint_config"),", refer to documentation ",(0,t.yg)("a",{parentName:"li",href:"././checkpoint_and_resume.md"},"checkpoint_and_resume")," for details."),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Evaluation Metrics")," \u2013 Recorded in TensorBoard and terminal."),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Rendered Frames")," \u2013 If ",(0,t.yg)("inlineCode",{parentName:"li"},"render_save_dir")," is configured, environment rendered frames will be saved in that directory, facilitating visualization of the interaction process.")),(0,t.yg)("hr",null),(0,t.yg)("p",null,(0,t.yg)("em",{parentName:"p"},"Happy experimenting!")))}u.isMDXComponent=!0},5680:(e,n,i)=>{i.d(n,{xA:()=>m,yg:()=>c});var a=i(6540);function t(e,n,i){return n in e?Object.defineProperty(e,n,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[n]=i,e}function r(e,n){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),i.push.apply(i,a)}return i}function l(e){for(var n=1;n<arguments.length;n++){var i=null!=arguments[n]?arguments[n]:{};n%2?r(Object(i),!0).forEach(function(n){t(e,n,i[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):r(Object(i)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(i,n))})}return e}function p(e,n){if(null==e)return{};var i,a,t=function(e,n){if(null==e)return{};var i,a,t={},r=Object.keys(e);for(a=0;a<r.length;a++)i=r[a],n.indexOf(i)>=0||(t[i]=e[i]);return t}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)i=r[a],n.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(t[i]=e[i])}return t}var o=a.createContext({}),g=function(e){var n=a.useContext(o),i=n;return e&&(i="function"==typeof e?e(n):l(l({},n),e)),i},m=function(e){var n=g(e.components);return a.createElement(o.Provider,{value:n},e.children)},s="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},y=a.forwardRef(function(e,n){var i=e.components,t=e.mdxType,r=e.originalType,o=e.parentName,m=p(e,["components","mdxType","originalType","parentName"]),s=g(i),y=t,c=s["".concat(o,".").concat(y)]||s[y]||u[y]||r;return i?a.createElement(c,l(l({ref:n},m),{},{components:i})):a.createElement(c,l({ref:n},m))});function c(e,n){var i=arguments,t=n&&n.mdxType;if("string"==typeof e||t){var r=i.length,l=new Array(r);l[0]=y;var p={};for(var o in n)hasOwnProperty.call(n,o)&&(p[o]=n[o]);p.originalType=e,p[s]="string"==typeof e?e:t,l[1]=p;for(var g=2;g<r;g++)l[g]=i[g];return a.createElement.apply(null,l)}return a.createElement.apply(null,i)}y.displayName="MDXCreateElement"}}]);