"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[6302],{5680:(e,t,n)=>{n.d(t,{xA:()=>m,yg:()=>h});var a=n(6540);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter(function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable})),n.push.apply(n,a)}return n}function i(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach(function(t){o(e,t,n[t])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach(function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))})}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):i(i({},t),e)),n},m=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},u="mdxType",c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},g=a.forwardRef(function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),u=p(n),g=o,h=u["".concat(l,".").concat(g)]||u[g]||c[g]||r;return n?a.createElement(h,i(i({ref:t},m),{},{components:n})):a.createElement(h,i({ref:t},m))});function h(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,i=new Array(r);i[0]=g;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[u]="string"==typeof e?e:o,i[1]=s;for(var p=2;p<r;p++)i[p]=n[p];return a.createElement.apply(null,i)}return a.createElement.apply(null,n)}g.displayName="MDXCreateElement"},9542:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>i,default:()=>c,frontMatter:()=>r,metadata:()=>s,toc:()=>p});var a=n(8168),o=(n(6540),n(5680));const r={},i="Prompt Generation Guide",s={unversionedId:"English/DevelopmentGuide/prompt_intro_en",id:"English/DevelopmentGuide/prompt_intro_en",title:"Prompt Generation Guide",description:'In the architecture of Large Language Model (LLM)-based Reinforcement Learning Agents, the Prompt serves as the sole medium for LLMs to interact with the environment. Unlike traditional agents that directly receive numerical states or output discrete action IDs, LLMs "perceive" the environment (observations) and "express" their decisions (actions) through prompts in text format.',source:"@site/docs/English/DevelopmentGuide/prompt_intro_en.md",sourceDirName:"English/DevelopmentGuide",slug:"/English/DevelopmentGuide/prompt_intro_en",permalink:"/ROLL/docs/English/DevelopmentGuide/prompt_intro_en",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/DevelopmentGuide/prompt_intro_en.md",tags:[],version:"current",lastUpdatedAt:1761384892,formattedLastUpdatedAt:"Oct 25, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Customer Env",permalink:"/ROLL/docs/English/DevelopmentGuide/customer_env_en"},next:{title:"Configuration Guide",permalink:"/ROLL/docs/English/QuickStart/config_guide"}},l={},p=[{value:"Core Concepts",id:"core-concepts",level:2},{value:"Prompt Generation Steps and Rules",id:"prompt-generation-steps-and-rules",level:2},{value:"Step 1: Initialization Conversation and Basic Instructions",id:"step-1-initialization-conversation-and-basic-instructions",level:3},{value:"Sokoban Example: Generating the First User Prompt",id:"sokoban-example-generating-the-first-user-prompt",level:4},{value:"Step 2: Iterate Through Environment History to Build Multi-turn Conversation Context",id:"step-2-iterate-through-environment-history-to-build-multi-turn-conversation-context",level:3},{value:"Sokoban Example: Multi-turn Prompt Construction",id:"sokoban-example-multi-turn-prompt-construction",level:4},{value:"Step 3: Apply Chat Template and Finally Generate Prompt Text",id:"step-3-apply-chat-template-and-finally-generate-prompt-text",level:3},{value:"Completed Prompt Generation Process",id:"completed-prompt-generation-process",level:2}],m={toc:p},u="wrapper";function c({components:e,...t}){return(0,o.yg)(u,(0,a.A)({},m,t,{components:e,mdxType:"MDXLayout"}),(0,o.yg)("h1",{id:"prompt-generation-guide"},"Prompt Generation Guide"),(0,o.yg)("p",null,"In the architecture of Large Language Model (LLM)-based Reinforcement Learning Agents, the ",(0,o.yg)("inlineCode",{parentName:"p"},"Prompt"),' serves as the sole medium for LLMs to interact with the environment. Unlike traditional agents that directly receive numerical states or output discrete action IDs, LLMs "perceive" the environment (observations) and "express" their decisions (actions) through prompts in text format.'),(0,o.yg)("h2",{id:"core-concepts"},"Core Concepts"),(0,o.yg)("p",null,"In our framework, the generation of prompts adheres to several key principles:"),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"LLM Input is Text: Whether the environment's original observation is an image, a grid, or another structure, it will ultimately be converted into a text format that LLMs can understand."),(0,o.yg)("li",{parentName:"ul"},"Prompts are Dynamic and Contextual: A prompt is not merely the current environmental observation; it also includes historical dialogue, previous actions, received rewards, and other information, forming a coherent conversational context."),(0,o.yg)("li",{parentName:"ul"},"Prompts are Structured Conversational Formats: Prompts typically follow the LLM's chat template (e.g., System/User/Assistant roles) to help the LLM better understand the intent of different parts."),(0,o.yg)("li",{parentName:"ul"},"Prompts can guide LLM's behavior: Through precise instructions, output format requirements, and Chain-of-Thought (CoT) prompting, prompts can guide the LLM to generate responses in the expected style.")),(0,o.yg)("p",null,"The generation of prompts is primarily managed by the ",(0,o.yg)("inlineCode",{parentName:"p"},"_format_messages")," method within the ",(0,o.yg)("inlineCode",{parentName:"p"},"EnvManager")," class. "),(0,o.yg)("h2",{id:"prompt-generation-steps-and-rules"},"Prompt Generation Steps and Rules"),(0,o.yg)("p",null,"The ",(0,o.yg)("inlineCode",{parentName:"p"},"_format_messages")," method is the core of prompt generation. It receives ",(0,o.yg)("inlineCode",{parentName:"p"},"env_output")," (containing current observations and historical information) and transforms it into LLM input based on a series of rules."),(0,o.yg)("h3",{id:"step-1-initialization-conversation-and-basic-instructions"},"Step 1: Initialization Conversation and Basic Instructions"),(0,o.yg)("p",null,"Prompt generation begins by constructing the skeleton of a conversation, including system instructions and the first user instruction."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'messages = [\n    # System Prompt: Defines the role and goal of the LLM\n    {"role": "system", "content": "You\'re a helpful assistant. You are a good game player. You are aiming to get high reward in the game."},\n    # First User Prompt: Contains the overall introduction to the environment and initial instructions\n    {"role": "user", "content": first_user_content}\n]\n')),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"System Prompt"),': This is a fixed instruction used to set the LLM\'s general role ("helpful assistant," "good game player") and overall goal ("aiming to get high reward"), which provides the LLM with a global guiding principle for action.'),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("strong",{parentName:"li"},"First User Prompt")," (",(0,o.yg)("inlineCode",{parentName:"li"},"first_user_content"),"): This is the most critical initialization part, which introduces the environment's rules, symbol meanings, available actions, and response format. Its content is pre-generated by the\xa0",(0,o.yg)("inlineCode",{parentName:"li"},"EnvManager._init_prefix_lookup"),"\xa0method, combining\xa0",(0,o.yg)("inlineCode",{parentName:"li"},"env_instruction"),",\xa0",(0,o.yg)("inlineCode",{parentName:"li"},"grid_vocab"),",\xa0",(0,o.yg)("inlineCode",{parentName:"li"},"action_lookup")," from the environment configuration. ")),(0,o.yg)("h4",{id:"sokoban-example-generating-the-first-user-prompt"},"Sokoban Example: Generating the First User Prompt"),(0,o.yg)("p",null,"Assume the\xa0",(0,o.yg)("inlineCode",{parentName:"p"},"SokobanEnvConfig"),"\xa0is configured as follows:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-yaml"},'env_instruction: "You are solving the Sokoban puzzle. You are the player and you need to push all boxes to targets. When you are right next to a box, you can push it by moving in the same direction. You cannot push a box through a wall, and you cannot pull a box. The answer must be one of action in a turn, format is <answer>Right</answer>"\n\ngrid_vocab:\n  "#": "wall"\n  "_": "empty"\n  "O": "target"\n  "\u221a": "box on target"\n  "X": "box"\n  "P": "player"\n  "S": "player on target"\n\naction_lookup:\n  1: "Up"\n  2: "Down"\n  3: "Left"\n  4: "Right"\n')),(0,o.yg)("p",null,"Then, ",(0,o.yg)("inlineCode",{parentName:"p"},"first_user_content")," (i.e., the first User Prompt) will be constructed as a string similar to the following:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"You are solving the Sokoban puzzle. You are the player and you need to push all boxes to targets. When you are right next to a box, you can push it by moving in the same direction. You cannot push a box through a wall, and you cannot pull a box. The answer must be one of action in a turn, format is <answer>Right</answer>\n\nThe meaning of each symbol in the state is:\n#: wall, _: empty, O: target, \u221a: box on target, X: box, P: player, S: player on target\n\nYour available actions are:\nUp, Down, Left, Right\n")),(0,o.yg)("p",null,"This Prompt block comprehensively describes the rules of the Sokoban game, the meaning of visual symbols, and the executable actions to the LLM, providing a foundational understanding for subsequent decision-making."),(0,o.yg)("h3",{id:"step-2-iterate-through-environment-history-to-build-multi-turn-conversation-context"},"Step 2: Iterate Through Environment History to Build Multi-turn Conversation Context"),(0,o.yg)("p",null,"After the initial Prompt, ",(0,o.yg)("inlineCode",{parentName:"p"},"_format_messages")," will iterate through ",(0,o.yg)("inlineCode",{parentName:"p"},"env_output['history']"),", adding observations, LLM responses, and rewards from each previous step to the conversation, forming a continuous context."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'# Iterate through environment history to build multi-turn conversation Prompt\nfor idx, content in enumerate(env_output["history"]):\n    # 1. Add turn number\n    messages[-1]["content"] += f"\\nTurn {idx + 1}:\\n"\n\n    # 2. Process environment state\n    if "state" in content:\n        FORMAT_PROMPT = "<think> [Your thoughts] </think> <answer> [your answer] </answer>" if self.pipeline_config.enable_think else "<answer> [your answer] </answer>"\n        LENGTH_PROMPT = f"Max response length: {self.env_config_lookup[env_output[\'env_id\']][\'max_tokens\']} words (tokens)."\n        messages[-1]["content"] += (\n            f"State:\\n{content[\'state\']}\\n"\n            f"You have {content[\'actions_left\']} actions left. "\n            f"Always output: {FORMAT_PROMPT} with no extra text."\n            f"Strictly follow this format, history response that do not follow the format will be set as \'INVALID\'. {LENGTH_PROMPT}\\n"\n            f"Decide the next action:\\n"\n        )\n    \n    # 3. Process LLM\'s response\n    if "llm_raw_response" in content:\n        messages.append({"role": "assistant", "content": content["llm_response"]})\n\n    # 4. Process reward\n    if "reward" in content:\n        messages.append({"role": "user", "content": f"Reward:\\n{content[\'reward\']}\\n"})\n')),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},"Turn Number: ",(0,o.yg)("inlineCode",{parentName:"li"},"\\nTurn {idx + 1}:\\n"),"\xa0explicitly labels the current turn of the conversation, helping the LLM understand the temporal sequence. "),(0,o.yg)("li",{parentName:"ul"},"Environment State: The environmental observation for the current turn. For Sokoban, this is the grid layout in text form."),(0,o.yg)("li",{parentName:"ul"},"Actions Remaining: ",(0,o.yg)("inlineCode",{parentName:"li"},"You have {content['actions_left']} actions left")," informs the LLM about the action limits for the current turn, aiding long-term planning."),(0,o.yg)("li",{parentName:"ul"},"Forced Output Format\uff1aUsually includes ","[Your thoughts][your answer]"," (if ",(0,o.yg)("inlineCode",{parentName:"li"},"enable_think")," = true) or ","[your answer]",", which compels the LLM to return its thoughts and final action in a structured style."),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("inlineCode",{parentName:"li"},"LENGTH_PROMPT"),": Hints at the maximum length for the LLM's response."),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("inlineCode",{parentName:"li"},"Strictly follow this format..."),": Emphasizes the importance of the format and warns that non-conforming responses will be marked as 'INVALID'."),(0,o.yg)("li",{parentName:"ul"},"LLM Response (",(0,o.yg)("inlineCode",{parentName:"li"},"Assistant")," role): The action generated by the LLM in the previous turn is added to the history as an Assistant message. "),(0,o.yg)("li",{parentName:"ul"},"Reward (",(0,o.yg)("inlineCode",{parentName:"li"},"User")," role): The reward feedback from the environment for the LLM's previous action is added to the history as a User message, providing an RL signal.")),(0,o.yg)("h4",{id:"sokoban-example-multi-turn-prompt-construction"},"Sokoban Example: Multi-turn Prompt Construction"),(0,o.yg)("p",null,"Assume the initial state of the environment is:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"#####\n#__O#  <- Target O\n#P_X#  <- Player P, Box X\n#___#\n#####\n")),(0,o.yg)("ol",null,(0,o.yg)("li",{parentName:"ol"},"Turn 1 (LLM receives Prompt for the first time)")),(0,o.yg)("p",null,"Before the LLM generates its first action, the Prompt it receives might look like this (simplified format, actual conversion uses ",(0,o.yg)("inlineCode",{parentName:"p"},"apply_chat_template"),"):"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"<|im_start|>system\nYou're a helpful assistant. You are a good game player. You are aiming to get high reward in the game.<|im_end|>\n<|im_start|>user\nYou are solving the Sokoban puzzle. You are the player and you need to push all boxes to targets. When you are right next to a box, you can push it by moving in the same direction. You cannot push a box through a wall, and you cannot pull a box. The answer must be one of action in a turn, format is <answer>Right</answer>\n\nThe meaning of each symbol in the state is:\n#: wall, _: empty, O: target, \u221a: box on target, X: box, P: player, S: player on target\n\nYour available actions are:\nUp, Down, Left, Right\n\nTurn 1:\nState:\n#####\n#__O#  \n#P_X#  \n#___#\n#####\nYou have 100 actions left. Always output: <answer> [your answer] </answer> with no extra text. Strictly follow this format, history response that do not follow the format will be set as 'INVALID'. Max response length: 100 words (tokens).\nDecide the next action:<|im_end|>\n<|im_start|>assistant\n")),(0,o.yg)("p",null,"The LLM might generate ",(0,o.yg)("inlineCode",{parentName:"p"},"<answer>Right</answer>")),(0,o.yg)("ol",{start:2},(0,o.yg)("li",{parentName:"ol"},"Turn 2 (LLM receives new state and reward)")),(0,o.yg)("p",null,"Assume the LLM chose Right. After the environment responds, the box is pushed one cell to the right, and the reward is -0.1. The new state is:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"#####\n#__O#\n#_PX#\n#___#\n#####\n")),(0,o.yg)("p",null,"At this point, the LLM will receive a Prompt containing all interactions from the first turn:"),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre"},"<|im_start|>system\nYou're a helpful assistant. You are a good game player. You are aiming to get high reward in the game.<|im_end|>\n<|im_start|>user\nYou are solving the Sokoban puzzle. You are the player and you need to push all boxes to targets. When you are right next to a box, you can push it by moving in the same direction. You cannot push a box through a wall, and you cannot pull a box. The answer must be one of action in a turn, format is <answer>Right</answer>\n\nThe meaning of each symbol in the state is:\n#: wall, _: empty, O: target, \u221a: box on target, X: box, P: player, S: player on target\n\nYour available actions are:\nUp, Down, Left, Right\n\nTurn 1:\nState:\n#####\n#__O#  \n#P_X#  \n#___#\n#####\nYou have 100 actions left. Always output: <answer> [your answer] </answer> with no extra text. Strictly follow this format, history response that do not follow the format will be set as 'INVALID'. Max response length: 100 words (tokens).\nDecide the next action:<|im_end|>\n<|im_start|>assistant\n<answer>Right</answer><|im_end|>\n<|im_start|>user\nReward:\n-0.1\n<|im_end|>\n<|im_start|>user\nTurn 2:\nState:\n#####\n#__O#\n#_PX#\n#___#\n#####\nYou have 99 actions left. Always output: <answer> [your answer] </answer> with no extra text. Strictly follow this format, history response that do not follow the format will be set as 'INVALID'. Max response length: 100 words (tokens).\nDecide the next action:<|im_end|>\n<|im_start|>assistant\n")),(0,o.yg)("p",null,"In this way, the LLM can see the completed conversation history each time, including its own decisions and the feedback from the environment, which is crucial for learning and long-term planning."),(0,o.yg)("h3",{id:"step-3-apply-chat-template-and-finally-generate-prompt-text"},"Step 3: Apply Chat Template and Finally Generate Prompt Text"),(0,o.yg)("p",null,"The final step is to convert the constructed messages list into a single string format of the Prompt that the LLM actually accepts."),(0,o.yg)("pre",null,(0,o.yg)("code",{parentName:"pre",className:"language-python"},'# Apply chat template to generate final Prompt text\nif self.processor: # For multi-modal models using ProcessorMixin\n    text = self.processor.apply_chat_template(messages, add_generation_prompt=(not prepare_for_update), tokenize=False)\nelse: # For text-only models using PreTrainedTokenizer\n    text = self.tokenizer.apply_chat_template(messages, add_generation_prompt=(not prepare_for_update), tokenize=False)\n\n# Force LLM to generate specific starting token (in inference mode)\nif not prepare_for_update:\n    if self.pipeline_config.enable_think:\n        text += "<think>" # Force LLM to think before answering\n    else:\n        text += "<answer>" # Force LLM to answer\n\n# Clean up special tokens\ntext = text.replace("<|im_end|>\\n", "<|im_end|>")\n')),(0,o.yg)("ul",null,(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("inlineCode",{parentName:"li"},"apply_chat_template"),"\uff1aThis is a method provided by the Hugging Face transformers library. It converts the messages list into a flattened string according to the specific format of the LLM being used (e.g., Qwen's ",(0,o.yg)("inlineCode",{parentName:"li"},"<|im_start|>role\\ncontent<|im_end|>")," structure)."),(0,o.yg)("li",{parentName:"ul"},(0,o.yg)("inlineCode",{parentName:"li"},"add_generation_prompt"),"\uff1aIn inference mode (not prepare_for_update), this parameter typically adds a special token to the end of the Prompt, such as ",(0,o.yg)("inlineCode",{parentName:"li"},"<|im_start|>assistant\\n"),", explicitly telling the LLM that it is now its turn to generate as the assistant role."),(0,o.yg)("li",{parentName:"ul"},"Force the Generation of Starting Token: When the LLM performs inference (generates a response), to ensure its output strictly follow the predefined format, we add a specific starting token, such as ",(0,o.yg)("inlineCode",{parentName:"li"},"<think>")," or ",(0,o.yg)("inlineCode",{parentName:"li"},"<answer>"),', to the end of the Prompt. This is a technique known as "Prompt Injection" or "Conditional Generation."',(0,o.yg)("ul",{parentName:"li"},(0,o.yg)("li",{parentName:"ul"},"Guide LLM to Continue: The essence of an LLM is to predict the next most probable token in a given sequence. When we place a marker like ",(0,o.yg)("inlineCode",{parentName:"li"},"<answer>")," at the end of the Prompt, the LLM treats it as an incomplete sequence and naturally attempts to continue generating content after this marker."),(0,o.yg)("li",{parentName:"ul"},"Enforce Format Adherence: If the Prompt has clearly specified that the response must be in the format ",(0,o.yg)("inlineCode",{parentName:"li"},"<answer>[your answer]</answer>"),", then by placing ",(0,o.yg)("inlineCode",{parentName:"li"},"</answer>"),' at the end of the Prompt, we are effectively pre-filling a part of the response format. Upon receiving this incomplete format, the LLM is "forced" to generate ',(0,o.yg)("inlineCode",{parentName:"li"},"[your answer]")," part and ultimately complete the ",(0,o.yg)("inlineCode",{parentName:"li"},"</answer>")," tag\u3002")))),(0,o.yg)("h2",{id:"completed-prompt-generation-process"},"Completed Prompt Generation Process"),(0,o.yg)("ol",null,(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("strong",{parentName:"li"},"Environment Configuration"),"\uff08",(0,o.yg)("inlineCode",{parentName:"li"},"SokobanEnvConfig"),"\uff09: Defines the static information of the environment (instructions, symbol meanings, action names). "),(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("inlineCode",{parentName:"li"},"_init_prefix_lookup"),": During ",(0,o.yg)("inlineCode",{parentName:"li"},"EnvManager")," initialization, combines the static information from the environment configuration into ",(0,o.yg)("inlineCode",{parentName:"li"},"first_user_content"),"."),(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("inlineCode",{parentName:"li"},"_format_messages"),"\uff1a\na. Called when starting a new turn or receiving new environmental feedback.\nb. Use ",(0,o.yg)("inlineCode",{parentName:"li"},"System Prompt")," and ",(0,o.yg)("inlineCode",{parentName:"li"},"first_user_content")," as the beginning of the conversation.\nc. Iterates through ",(0,o.yg)("inlineCode",{parentName:"li"},"env_output['history']"),", successively adding turn numbers, environmental states, remaining actions, historical LLM responses, rewards, and other dynamic information.\nd. Repeats mandatory format requirements after the environmental state in each turn.\ne. Use ",(0,o.yg)("inlineCode",{parentName:"li"},"tokenizer.apply_chat_template"),"  to convert the constructed structured ",(0,o.yg)("inlineCode",{parentName:"li"},"messages")," list into the final Prompt string acceptable by the LLM.\nf. In inference mode, add the forced generation starting token ",(0,o.yg)("inlineCode",{parentName:"li"},"<think>")," or ",(0,o.yg)("inlineCode",{parentName:"li"},"<answer>"),"."),(0,o.yg)("li",{parentName:"ol"},(0,o.yg)("strong",{parentName:"li"},"LLM Receives the Prompt"),"\uff1aReceives this carefully constructed Prompt string, performs inference, and generates a response.")),(0,o.yg)("p",null,"Through this layered, structured, and dynamic Prompt generation mechanism, our framework effectively combines complex environments with the powerful language capabilities of LLMs, enabling them to understand tasks, perceive environments, learn rules, and execute complex operations."))}c.isMDXComponent=!0}}]);