"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[2785],{5680:(e,n,a)=>{a.d(n,{xA:()=>s,yg:()=>y});var i=a(6540);function t(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function r(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),a.push.apply(a,i)}return a}function l(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?r(Object(a),!0).forEach(function(n){t(e,n,a[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):r(Object(a)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))})}return e}function p(e,n){if(null==e)return{};var a,i,t=function(e,n){if(null==e)return{};var a,i,t={},r=Object.keys(e);for(i=0;i<r.length;i++)a=r[i],n.indexOf(a)>=0||(t[a]=e[a]);return t}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(i=0;i<r.length;i++)a=r[i],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(t[a]=e[a])}return t}var o=i.createContext({}),g=function(e){var n=i.useContext(o),a=n;return e&&(a="function"==typeof e?e(n):l(l({},n),e)),a},s=function(e){var n=g(e.components);return i.createElement(o.Provider,{value:n},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},d=i.forwardRef(function(e,n){var a=e.components,t=e.mdxType,r=e.originalType,o=e.parentName,s=p(e,["components","mdxType","originalType","parentName"]),m=g(a),d=t,y=m["".concat(o,".").concat(d)]||m[d]||u[d]||r;return a?i.createElement(y,l(l({ref:n},s),{},{components:a})):i.createElement(y,l({ref:n},s))});function y(e,n){var a=arguments,t=n&&n.mdxType;if("string"==typeof e||t){var r=a.length,l=new Array(r);l[0]=d;var p={};for(var o in n)hasOwnProperty.call(n,o)&&(p[o]=n[o]);p.originalType=e,p[m]="string"==typeof e?e:t,l[1]=p;for(var g=2;g<r;g++)l[g]=a[g];return i.createElement.apply(null,l)}return i.createElement.apply(null,a)}d.displayName="MDXCreateElement"},6775:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>r,metadata:()=>p,toc:()=>g});var i=a(8168),t=(a(6540),a(5680));const r={},l="DPO Pipeline",p={unversionedId:"English/UserGuide/pipeline/dpo_pipeline_start",id:"English/UserGuide/pipeline/dpo_pipeline_start",title:"DPO Pipeline",description:"Table of Contents",source:"@site/docs/English/UserGuide/pipeline/dpo_pipeline_start.md",sourceDirName:"English/UserGuide/pipeline",slug:"/English/UserGuide/pipeline/dpo_pipeline_start",permalink:"/ROLL/docs/English/UserGuide/pipeline/dpo_pipeline_start",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/pipeline/dpo_pipeline_start.md",tags:[],version:"current",lastUpdatedAt:1761384892,formattedLastUpdatedAt:"Oct 25, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Distill Pipeline",permalink:"/ROLL/docs/English/UserGuide/pipeline/distill_pipeline_start"},next:{title:"RLVR Pipeline",permalink:"/ROLL/docs/English/UserGuide/pipeline/rlvr_pipeline_start"}},o={},g=[{value:"\u2728\ufe0fOverview",id:"\ufe0foverview",level:2},{value:"\u2728\ufe0fCore Components",id:"\ufe0fcore-components",level:2},{value:"Main Module (<code>DPOPipeline</code>)",id:"main-module-dpopipeline",level:3},{value:"Configuration File (<code>DPOConfig</code>)",id:"configuration-file-dpoconfig",level:3},{value:"Configuration File Structure and Organization",id:"configuration-file-structure-and-organization",level:4},{value:"\u2728\ufe0fData Preparation",id:"\ufe0fdata-preparation",level:2},{value:"Data Format",id:"data-format",level:3},{value:"Required Columns",id:"required-columns",level:3},{value:"\u2728\ufe0fRunning the Pipeline",id:"\ufe0frunning-the-pipeline",level:2},{value:"Method 1: Using Python Launcher Script",id:"method-1-using-python-launcher-script",level:3},{value:"Method 2: Using Helper Shell Scripts",id:"method-2-using-helper-shell-scripts",level:3},{value:"\u2728\ufe0fStep-by-Step Example",id:"\ufe0fstep-by-step-example",level:2},{value:"Step 1: Configure Settings",id:"step-1-configure-settings",level:3},{value:"Step 2: Prepare Environment and Dependencies",id:"step-2-prepare-environment-and-dependencies",level:3},{value:"Step 3: Launch the Pipeline",id:"step-3-launch-the-pipeline",level:3},{value:"Step 4: Monitoring",id:"step-4-monitoring",level:3},{value:"Step 5: Outputs and Results",id:"step-5-outputs-and-results",level:3}],s={toc:g},m="wrapper";function u({components:e,...n}){return(0,t.yg)(m,(0,i.A)({},s,n,{components:e,mdxType:"MDXLayout"}),(0,t.yg)("h1",{id:"dpo-pipeline"},"DPO Pipeline"),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Table of Contents")),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#DPO-pipeline"},"DPO Pipeline"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#%EF%B8%8Foverview"},"\u2728\ufe0fOverview")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#%EF%B8%8Fcore-components"},"\u2728\ufe0fCore Components"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#main-module-DPOPipeline"},"Main Module (",(0,t.yg)("inlineCode",{parentName:"a"},"DPOPipeline"),")")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#configuration-file-DPOConfig"},"Configuration File (",(0,t.yg)("inlineCode",{parentName:"a"},"DPOConfig"),")"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#configuration-file-structure-and-organization"},"Configuration File Structure and Organization")))))),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#%EF%B8%8Fdata-preparation"},"\u2728\ufe0fData Preparation"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#data-format"},"Data Format"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#common-data-fields"},"Common Data Fields")))))),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#%EF%B8%8Frunning-the-pipeline"},"\u2728\ufe0fRunning the Pipeline"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#method-1-using-python-launcher-script"},"Method 1: Using Python Launcher Script")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#method-2-using-helper-shell-scripts"},"Method 2: Using Helper Shell Scripts")))),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#%EF%B8%8Fstep-by-step-example"},"\u2728\ufe0fStep-by-Step Example"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#step-1-configure-settings"},"Step 1: Configure Settings")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#step-2-prepare-environment-and-dependencies"},"Step 2: Prepare Environment and Dependencies")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#step-3-launch-the-pipeline"},"Step 3: Launch the Pipeline")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#step-4-monitoring"},"Step 4: Monitoring")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("a",{parentName:"li",href:"#step-5-outputs-and-results"},"Step 5: Outputs and Results"))))))),(0,t.yg)("hr",null),(0,t.yg)("h2",{id:"\ufe0foverview"},"\u2728\ufe0fOverview"),(0,t.yg)("p",null," This pipeline offers the following core advantages:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Various DPO losses"),": Support for training the model with different DPO losses and finer-grained configuration via the corresponding parameters.")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Comprehensive Performance Monitoring"),": Fine-grained metric tracking system that monitors performance metrics, providing comprehensive visualization and analysis capabilities for the model training process.")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Efficient Distributed Computing"),": Leverages the ",(0,t.yg)("a",{parentName:"p",href:"https://www.ray.io/"},"Ray")," framework to implement efficient distributed training on large-scale GPU clusters, significantly improving training speed and resource utilization."))),(0,t.yg)("hr",null),(0,t.yg)("h2",{id:"\ufe0fcore-components"},"\u2728\ufe0fCore Components"),(0,t.yg)("h3",{id:"main-module-dpopipeline"},"Main Module (",(0,t.yg)("inlineCode",{parentName:"h3"},"DPOPipeline"),")"),(0,t.yg)("p",null,(0,t.yg)("inlineCode",{parentName:"p"},"DPOPipeline")," (located in ",(0,t.yg)("inlineCode",{parentName:"p"},"roll/pipeline/dpo/dpo_pipeline.py"),") is the primary coordinator for the entire DPO training process. It manages the complete training workflow, including:"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},"Initializing and managing distributed workers (Actor and Reference workers)."),(0,t.yg)("li",{parentName:"ul"},"Coordinating data collection and processing."),(0,t.yg)("li",{parentName:"ul"},"Executing model training steps."),(0,t.yg)("li",{parentName:"ul"},"Handling checkpoint saving."),(0,t.yg)("li",{parentName:"ul"},"Recording metrics and experiment tracking.")),(0,t.yg)("p",null,(0,t.yg)("strong",{parentName:"p"},"Source code"),": ",(0,t.yg)("inlineCode",{parentName:"p"},"roll/pipeline/dpo/dpo_pipeline.py")),(0,t.yg)("hr",null),(0,t.yg)("h3",{id:"configuration-file-dpoconfig"},"Configuration File (",(0,t.yg)("inlineCode",{parentName:"h3"},"DPOConfig"),")"),(0,t.yg)("p",null,(0,t.yg)("inlineCode",{parentName:"p"},"DPOConfig")," (defined in ",(0,t.yg)("inlineCode",{parentName:"p"},"roll/pipeline/dpo/dpo_config.py"),") is a Pydantic/dataclass-based configuration object used to specify all parameters for running the DPOPipeline. This configuration system is flexibly designed, supporting configuration via YAML files and managed using the Hydra framework."),(0,t.yg)("h4",{id:"configuration-file-structure-and-organization"},"Configuration File Structure and Organization"),(0,t.yg)("p",null,"Configuration files (such as ",(0,t.yg)("inlineCode",{parentName:"p"},"examples/qwen2.5-3B-dpo_megatron/dpo_config.yaml"),") are organized by functional modules, containing the following main sections:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Experiment Basic Settings")),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"exp_name"),": Experiment name, used to identify a specific training run"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"logging_dir"),": Path for saving log files"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"output_dir"),": Path for saving model checkpoints and output files"))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Training Control Parameters")),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"max_steps"),": Maximum number of training steps"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"save_steps"),": Frequency for saving model checkpoints"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"logging_steps"),": Frequency for recording training metrics"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"resume_from_checkpoint"),": Whether to continue training from a checkpoint. Set it to the checkpoint path if you want to resume; otherwise, set it to ",(0,t.yg)("inlineCode",{parentName:"li"},"False"),"."))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"DPO Algorithm Parameters")),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"ipo"),": Use IPO loss function"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"beta"),": Regulates the model's sensitivity to human preference data"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"label_smoothing"),": A regularization technique that reduces overfitting risk by softening the model's absolute confidence in labels"))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Worker Configuration"),"\nEach worker (",(0,t.yg)("inlineCode",{parentName:"p"},"actor_train"),", ",(0,t.yg)("inlineCode",{parentName:"p"},"reference"),") configuration contains:"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Model Parameters")," (",(0,t.yg)("inlineCode",{parentName:"li"},"model_args"),")",(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"model_type"),": Model type (e.g., ",(0,t.yg)("inlineCode",{parentName:"li"},"causal_lm"),")"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"dtype"),": Computation precision (e.g., ",(0,t.yg)("inlineCode",{parentName:"li"},"bf16"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"fp16"),")"),(0,t.yg)("li",{parentName:"ul"},"..."))),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Training Parameters")," (",(0,t.yg)("inlineCode",{parentName:"li"},"training_args"),")",(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"learning_rate"),": Learning rate"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"per_device_train_batch_size"),": Training batch size per device"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"gradient_accumulation_steps"),": Gradient accumulation steps"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"weight_decay"),": Weight decay coefficient"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"max_grad_norm"),": Gradient clipping threshold"),(0,t.yg)("li",{parentName:"ul"},"..."))),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Distributed Strategy")," (",(0,t.yg)("inlineCode",{parentName:"li"},"strategy_args"),")",(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"strategy_name"),": Distributed strategy to use (e.g., ",(0,t.yg)("inlineCode",{parentName:"li"},"megatron_train"),", ",(0,t.yg)("inlineCode",{parentName:"li"},"deepspeed_infer"),")"),(0,t.yg)("li",{parentName:"ul"},"Strategy-specific parameters: e.g., ",(0,t.yg)("inlineCode",{parentName:"li"},"tp_size")," (tensor parallelism size), ",(0,t.yg)("inlineCode",{parentName:"li"},"pp_size")," (pipeline parallelism size)"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"gpu_memory_utilization"),": GPU memory utilization (vLLM-specific)"))),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Device Mapping")," (",(0,t.yg)("inlineCode",{parentName:"li"},"device_mapping"),")",(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},"Specifies which GPU devices the worker should use")))))),(0,t.yg)("hr",null),(0,t.yg)("h2",{id:"\ufe0fdata-preparation"},"\u2728\ufe0fData Preparation"),(0,t.yg)("h3",{id:"data-format"},"Data Format"),(0,t.yg)("p",null,"The DPO pipeline expects the training data to be stored in ",(0,t.yg)("strong",{parentName:"p"},"JSON")," files."),(0,t.yg)("h3",{id:"required-columns"},"Required Columns"),(0,t.yg)("p",null,"Each data sample must contain a question, a chosen answer, and a rejected answer.\nIn the YAML file, use chosen_key and rejected_key to specify the corresponding field names in the dataset."),(0,t.yg)("p",null,"For example:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre"},'"instruction": "Select a color and provide some adjectives to describe it.",\n"input": "",\n"chosen": "The color is blue. Adjectives to describe it include calming, soothing, serene, and peaceful.",\n"rejected": "Red"\n')),(0,t.yg)("hr",null),(0,t.yg)("h2",{id:"\ufe0frunning-the-pipeline"},"\u2728\ufe0fRunning the Pipeline"),(0,t.yg)("h3",{id:"method-1-using-python-launcher-script"},"Method 1: Using Python Launcher Script"),(0,t.yg)("p",null,"The primary method uses the examples/start_dpo_pipeline.py script. This script leverages Hydra to load and manage configurations."),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Select or Create a Configuration File"),(0,t.yg)("br",{parentName:"p"}),"\n","Start with an example YAML (e.g., ",(0,t.yg)("inlineCode",{parentName:"p"},"examples/qwen2.5-3B-dpo_megatron/dpo_config.yaml"),") or create your own configuration.")),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Execute the Python Launcher Script")),(0,t.yg)("pre",{parentName:"li"},(0,t.yg)("code",{parentName:"pre",className:"language-bash"}," # Make sure you are in the root directory of the ROLL project\n # export PYTHONPATH=$(pwd):$PYTHONPATH\n \n python examples/start_dpo_pipeline.py \\\n        --config_path examples/qwen2.5-3B-dpo_megatron \\\n        --config_name dpo_config\n")),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"--config_path")," \u2013 Directory containing your YAML configuration."),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"--config_name")," \u2013 Filename (without ",(0,t.yg)("inlineCode",{parentName:"li"},".yaml"),").")))),(0,t.yg)("h3",{id:"method-2-using-helper-shell-scripts"},"Method 2: Using Helper Shell Scripts"),(0,t.yg)("p",null,"The ",(0,t.yg)("inlineCode",{parentName:"p"},"examples")," directory typically contains shell scripts that wrap the Python launcher."),(0,t.yg)("p",null,"Example structure:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},'#!/bin/bash\n# Example: examples/qwen2.5-3B-dpo_megatron/run_dpo_pipeline.sh\n\nCONFIG_NAME="dpo_config"\nCONFIG_PATH="examples/qwen2.5-3B-dpo_megatron"\n\n# Set environment variables and other configurations\n\npython examples/start_dpo_pipeline.py \\\n       --config_path $CONFIG_PATH \\\n       --config_name $CONFIG_NAME \\\n       "$@"   # Pass any additional arguments\n')),(0,t.yg)("p",null,"Run using:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"bash bash examples/qwen2.5-3B-dpo_megatron/run_dpo_pipeline.sh\n")),(0,t.yg)("hr",null),(0,t.yg)("h2",{id:"\ufe0fstep-by-step-example"},"\u2728\ufe0fStep-by-Step Example"),(0,t.yg)("h3",{id:"step-1-configure-settings"},"Step 1: Configure Settings"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},"File: ",(0,t.yg)("inlineCode",{parentName:"p"},"examples/qwen2.5-3B-dpo_megatron/dpo_config.yaml"),(0,t.yg)("br",{parentName:"p"}),"\n","Key sections include exp_name, seed, output_dir, model paths, and configurations for actor_train and reference.")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},"Pay special attention to these configuration sections:"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},"Data configuration: ",(0,t.yg)("inlineCode",{parentName:"li"},"actor_train.data_args.file_name")),(0,t.yg)("li",{parentName:"ul"},"Model configuration: ",(0,t.yg)("inlineCode",{parentName:"li"},"pretrain")," path"),(0,t.yg)("li",{parentName:"ul"},"Distributed strategies: ",(0,t.yg)("inlineCode",{parentName:"li"},"strategy_args")," and ",(0,t.yg)("inlineCode",{parentName:"li"},"device_mapping")," for each worker")))),(0,t.yg)("h3",{id:"step-2-prepare-environment-and-dependencies"},"Step 2: Prepare Environment and Dependencies"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},"Ensure all necessary dependencies are installed:"),(0,t.yg)("pre",{parentName:"li"},(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"pip install -r requirements.txt\n"))),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},"Verify that all model paths in the configuration are accessible.")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},"Prepare training datasets, ensuring they conform to the data format requirements described above."))),(0,t.yg)("h3",{id:"step-3-launch-the-pipeline"},"Step 3: Launch the Pipeline"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"python examples/start_dpo_pipeline.py \\\n      --config_path examples/qwen2.5-3B-dpo_megatron \\\n      --config_name dpo_config\n")),(0,t.yg)("h3",{id:"step-4-monitoring"},"Step 4: Monitoring"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Console Output")," \u2013 Observe Hydra, Ray, and pipeline logs.")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Log Files")," \u2013 Check the ",(0,t.yg)("inlineCode",{parentName:"p"},"logging_dir")," specified in the YAML.")),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"TensorBoard")),(0,t.yg)("pre",{parentName:"li"},(0,t.yg)("code",{parentName:"pre",className:"language-bash"},"tensorboard --logdir <your_log_dir>\n")))),(0,t.yg)("h3",{id:"step-5-outputs-and-results"},"Step 5: Outputs and Results"),(0,t.yg)("ul",null,(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Trained Models")," \u2013 Checkpoints are saved in the ",(0,t.yg)("inlineCode",{parentName:"li"},"output_dir"),"."),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("strong",{parentName:"li"},"Evaluation Metrics")," \u2013 Recorded in TensorBoard and the console.")),(0,t.yg)("hr",null),(0,t.yg)("p",null,(0,t.yg)("em",{parentName:"p"},"Happy experimenting!")))}u.isMDXComponent=!0}}]);