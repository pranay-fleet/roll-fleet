"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[3310],{3229:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/log_pipeline_start-1b28c489a3d9d8cc9fef5dc2aab7ead7.png"},3560:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/log_pipeline_complete-3e33d8f8e3007b5184d9e0ba1badb7df.png"},5680:(e,n,t)=>{t.d(n,{xA:()=>p,yg:()=>m});var r=t(6540);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function a(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,r)}return t}function o(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?a(Object(t),!0).forEach(function(n){i(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function l(e,n){if(null==e)return{};var t,r,i=function(e,n){if(null==e)return{};var t,r,i={},a=Object.keys(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(r=0;r<a.length;r++)t=a[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=r.createContext({}),c=function(e){var n=r.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):o(o({},n),e)),t},p=function(e){var n=c(e.components);return r.createElement(s.Provider,{value:n},e.children)},g="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},d=r.forwardRef(function(e,n){var t=e.components,i=e.mdxType,a=e.originalType,s=e.parentName,p=l(e,["components","mdxType","originalType","parentName"]),g=c(t),d=i,m=g["".concat(s,".").concat(d)]||g[d]||u[d]||a;return t?r.createElement(m,o(o({ref:n},p),{},{components:t})):r.createElement(m,o({ref:n},p))});function m(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var a=t.length,o=new Array(a);o[0]=d;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[g]="string"==typeof e?e:i,o[1]=l;for(var c=2;c<a;c++)o[c]=t[c];return r.createElement.apply(null,o)}return r.createElement.apply(null,t)}d.displayName="MDXCreateElement"},6967:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/log_pipeline_in_training-24ad92a3612a1c18937a60ddde03f385.png"},7653:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>l,toc:()=>c});var r=t(8168),i=(t(6540),t(5680));const a={},o="Quick Start: Single-Node Deployment Guide",l={unversionedId:"English/QuickStart/single_node_quick_start",id:"English/QuickStart/single_node_quick_start",title:"Quick Start: Single-Node Deployment Guide",description:"Environment Preparation",source:"@site/docs/English/QuickStart/single_node_quick_start.md",sourceDirName:"English/QuickStart",slug:"/English/QuickStart/single_node_quick_start",permalink:"/ROLL/docs/English/QuickStart/single_node_quick_start",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/QuickStart/single_node_quick_start.md",tags:[],version:"current",lastUpdatedAt:1761386293,formattedLastUpdatedAt:"Oct 25, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Frequently Asked Questions (Q&A)",permalink:"/ROLL/docs/English/QuickStart/qa_issues"},next:{title:"Tool Use Guide",permalink:"/ROLL/docs/English/UserGuide/agentic/Tool_Use"}},s={},c=[{value:"Environment Preparation",id:"environment-preparation",level:2},{value:"Environment Configuration",id:"environment-configuration",level:2},{value:"Pipeline Execution",id:"pipeline-execution",level:2},{value:"Reference: Single V100 GPU Memory Configuration Key Points",id:"reference-single-v100-gpu-memory-configuration-key-points",level:2}],p={toc:c},g="wrapper";function u({components:e,...n}){return(0,i.yg)(g,(0,r.A)({},p,n,{components:e,mdxType:"MDXLayout"}),(0,i.yg)("h1",{id:"quick-start-single-node-deployment-guide"},"Quick Start: Single-Node Deployment Guide"),(0,i.yg)("h2",{id:"environment-preparation"},"Environment Preparation"),(0,i.yg)("ol",null,(0,i.yg)("li",{parentName:"ol"},"Purchase a machine equipped with GPU and install GPU drivers synchronously"),(0,i.yg)("li",{parentName:"ol"},"Connect to the GPU instance remotely and enter the machine terminal"),(0,i.yg)("li",{parentName:"ol"},"Run the following command to install the Docker environment and NVIDIA container toolkit")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-shell"},"curl -fsSL https://github.com/alibaba/ROLL/blob/main/scripts/install_docker_nvidia_container_toolkit.sh | sudo bash\n")),(0,i.yg)("h2",{id:"environment-configuration"},"Environment Configuration"),(0,i.yg)("p",null,"Choose your desired Docker image from the ",(0,i.yg)("a",{parentName:"p",href:"https://alibaba.github.io/ROLL/docs/English/QuickStart/image_address"},"image addresses"),". The following example uses ",(0,i.yg)("em",{parentName:"p"},"torch2.6.0 + vLLM0.8.4")),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-shell"},"# 1. Start a Docker container with GPU support, expose container ports, and keep the container running\nsudo docker run -dit \\\n  --gpus all \\\n  -p 9001:22 \\\n  --ipc=host \\\n  --shm-size=10gb \\\n  roll-registry.cn-hangzhou.cr.aliyuncs.com/roll/pytorch:nvcr-24.05-py3-torch260-vllm084 \\\n  /bin/bash\n\n# 2. Enter the Docker container\n#    You can use the `sudo docker ps` command to find the running container ID or name.\nsudo docker exec -it <container_id> /bin/bash\n\n# 3. Verify that GPUs are visible\nnvidia-smi\n\n# 4. Clone the project code\ngit clone https://github.com/alibaba/ROLL.git\n\n# 5. Install project dependencies (choose the requirements file corresponding to your image)\ncd ROLL\npip install -r requirements_torch260_vllm.txt -i https://mirrors.aliyun.com/pypi/simple/\n")),(0,i.yg)("h2",{id:"pipeline-execution"},"Pipeline Execution"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-shell"},"bash examples/agentic_demo/run_agentic_pipeline_frozen_lake_single_node_demo.sh\n")),(0,i.yg)("p",null,"Example log screenshots during pipeline execution:\n",(0,i.yg)("img",{alt:"log_pipeline_start",src:t(3229).A,width:"2868",height:"650"})),(0,i.yg)("p",null,(0,i.yg)("img",{alt:"log_pipeline_in_training",src:t(6967).A,width:"2876",height:"904"})),(0,i.yg)("p",null,(0,i.yg)("img",{alt:"log_pipeline_complete",src:t(3560).A,width:"1904",height:"206"})),(0,i.yg)("h2",{id:"reference-single-v100-gpu-memory-configuration-key-points"},"Reference: Single V100 GPU Memory Configuration Key Points"),(0,i.yg)("pre",null,(0,i.yg)("code",{parentName:"pre",className:"language-yaml"},"# Reduce the expected number of GPUs from 8 to the 1 V100 you actually have\nnum_gpus_per_node: 1 \n# Training processes now only map to GPU 0\nactor_train.device_mapping: list(range(0,1))\n# Inference processes now only map to GPU 0\nactor_infer.device_mapping: list(range(0,1))\n# Reference model processes now only map to GPU 0\nreference.device_mapping: list(range(0,1))\n\n# Significantly reduce the batch size during Rollout/Validation stages to prevent out-of-memory errors when a single GPU processes large batches\nrollout_batch_size: 16\nval_batch_size: 16\n\n# V100 has better native support for FP16 than BF16 (unlike A100/H100). Switching to FP16 can improve compatibility and stability while saving GPU memory.\nactor_train.model_args.dtype: fp16\nactor_infer.model_args.dtype: fp16\nreference.model_args.dtype: fp16\n\n# Switch the large model training framework from DeepSpeed to Megatron-LM, where parameters can be sent in batches for faster execution\nstrategy_name: megatron_train\nstrategy_config:\n  tensor_model_parallel_size: 1\n  pipeline_model_parallel_size: 1\n  expert_model_parallel_size: 1\n  use_distributed_optimizer: true\n  recompute_granularity: full\n\n# In Megatron training, the global training batch size is per_device_train_batch_size * gradient_accumulation_steps * world_size\nactor_train.training_args.per_device_train_batch_size: 1\nactor_train.training_args.gradient_accumulation_steps: 16  \n\n# Reduce the maximum number of actions per trajectory to make each Rollout trajectory shorter, reducing the length of LLM-generated content\nmax_actions_per_traj: 10    \n\n# Reduce the number of parallel training environment groups and validation environment groups to accommodate single GPU resources\ntrain_env_manager.env_groups: 1\ntrain_env_manager.n_groups: 1\nval_env_manager.env_groups: 2\nval_env_manager.n_groups: [1, 1]\nval_env_manager.tags: [SimpleSokoban, FrozenLake]\n\n# Reduce the total number of training steps to run a complete training process faster for quick debugging\nmax_steps: 100\n")))}u.isMDXComponent=!0}}]);