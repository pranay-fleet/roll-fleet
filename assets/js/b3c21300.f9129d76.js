"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[3269],{5680:(e,o,t)=>{t.d(o,{xA:()=>d,yg:()=>f});var n=t(6540);function r(e,o,t){return o in e?Object.defineProperty(e,o,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[o]=t,e}function a(e,o){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);o&&(n=n.filter(function(o){return Object.getOwnPropertyDescriptor(e,o).enumerable})),t.push.apply(t,n)}return t}function i(e){for(var o=1;o<arguments.length;o++){var t=null!=arguments[o]?arguments[o]:{};o%2?a(Object(t),!0).forEach(function(o){r(e,o,t[o])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):a(Object(t)).forEach(function(o){Object.defineProperty(e,o,Object.getOwnPropertyDescriptor(t,o))})}return e}function l(e,o){if(null==e)return{};var t,n,r=function(e,o){if(null==e)return{};var t,n,r={},a=Object.keys(e);for(n=0;n<a.length;n++)t=a[n],o.indexOf(t)>=0||(r[t]=e[t]);return r}(e,o);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)t=a[n],o.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var s=n.createContext({}),c=function(e){var o=n.useContext(s),t=o;return e&&(t="function"==typeof e?e(o):i(i({},o),e)),t},d=function(e){var o=c(e.components);return n.createElement(s.Provider,{value:o},e.children)},m="mdxType",p={inlineCode:"code",wrapper:function(e){var o=e.children;return n.createElement(n.Fragment,{},o)}},u=n.forwardRef(function(e,o){var t=e.components,r=e.mdxType,a=e.originalType,s=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),m=c(t),u=r,f=m["".concat(s,".").concat(u)]||m[u]||p[u]||a;return t?n.createElement(f,i(i({ref:o},d),{},{components:t})):n.createElement(f,i({ref:o},d))});function f(e,o){var t=arguments,r=o&&o.mdxType;if("string"==typeof e||r){var a=t.length,i=new Array(a);i[0]=u;var l={};for(var s in o)hasOwnProperty.call(o,s)&&(l[s]=o[s]);l.originalType=e,l[m]="string"==typeof e?e:r,i[1]=l;for(var c=2;c<a;c++)i[c]=t[c];return n.createElement.apply(null,i)}return n.createElement.apply(null,t)}u.displayName="MDXCreateElement"},9902:(e,o,t)=>{t.r(o),t.d(o,{assets:()=>s,contentTitle:()=>i,default:()=>p,frontMatter:()=>a,metadata:()=>l,toc:()=>c});var n=t(8168),r=(t(6540),t(5680));const a={},i="GPU Time-Division Multiplexing Control Guide",l={unversionedId:"English/UserGuide/offload_reload_control",id:"English/UserGuide/offload_reload_control",title:"GPU Time-Division Multiplexing Control Guide",description:"The ROLL framework implements GPU time-division multiplexing functionality, which allows flexible sharing of GPU resources between different roles through offload/reload capabilities. This document will provide detailed instructions on how to use this feature.",source:"@site/docs/English/UserGuide/offload_reload_control.md",sourceDirName:"English/UserGuide",slug:"/English/UserGuide/offload_reload_control",permalink:"/ROLL/docs/English/UserGuide/offload_reload_control",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/offload_reload_control.md",tags:[],version:"current",lastUpdatedAt:1761384892,formattedLastUpdatedAt:"Oct 25, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Converting MCoreAdapter Models to Hugging Face Format",permalink:"/ROLL/docs/English/UserGuide/megatron_convert_2_hf"},next:{title:"Comprehensive Guide: Using the Agentic Part of ROLL",permalink:"/ROLL/docs/English/UserGuide/pipeline/agent_pipeline_start"}},s={},c=[{value:"Time-Division Multiplexing Overview",id:"time-division-multiplexing-overview",level:2},{value:"Offload/Reload Control Mechanism",id:"offloadreload-control-mechanism",level:2},{value:"Automatic Control",id:"automatic-control",level:3},{value:"Manual Control",id:"manual-control",level:3},{value:"Usage Example",id:"usage-example",level:2},{value:"Context Manager Support",id:"context-manager-support",level:2},{value:"Memory Monitoring",id:"memory-monitoring",level:2},{value:"Usage Recommendations",id:"usage-recommendations",level:2}],d={toc:c},m="wrapper";function p({components:e,...o}){return(0,r.yg)(m,(0,n.A)({},d,o,{components:e,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"gpu-time-division-multiplexing-control-guide"},"GPU Time-Division Multiplexing Control Guide"),(0,r.yg)("p",null,"The ROLL framework implements GPU time-division multiplexing functionality, which allows flexible sharing of GPU resources between different roles through offload/reload capabilities. This document will provide detailed instructions on how to use this feature."),(0,r.yg)("h2",{id:"time-division-multiplexing-overview"},"Time-Division Multiplexing Overview"),(0,r.yg)("p",null,"In the ROLL framework, different roles (such as actor_train, actor_infer, critic, reference, and rewards) may need to use the same GPU resources. To improve resource utilization, the framework implements GPU time-division multiplexing functionality, which allows model states to be switched between GPU and CPU at different time points."),(0,r.yg)("h2",{id:"offloadreload-control-mechanism"},"Offload/Reload Control Mechanism"),(0,r.yg)("h3",{id:"automatic-control"},"Automatic Control"),(0,r.yg)("p",null,"Taking RLVRPipeline as an example, the framework automatically manages the offload and reload of model states:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},"# Example in rlvr_pipeline.py\nref_log_probs = self.reference.compute_log_probs(batch, blocking=True)\n")),(0,r.yg)("p",null,"By default, when executing RPC calls to a worker, the framework will first reload the GPU-related state of the current worker onto the GPU, and after execution is completed, it will offload the state to memory."),(0,r.yg)("h3",{id:"manual-control"},"Manual Control"),(0,r.yg)("p",null,"You can also manually intervene in model state management by setting ",(0,r.yg)("inlineCode",{parentName:"p"},'batch.meta_info["is_offload_states"]'),":"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},"# Example in rlvr_pipeline.py\nself.actor_train.offload_states(blocking=True)\n")),(0,r.yg)("p",null,"When ",(0,r.yg)("inlineCode",{parentName:"p"},"is_offload_states")," is set to ",(0,r.yg)("inlineCode",{parentName:"p"},"False"),", the model state will not be automatically offloaded to CPU after the RPC call is completed, and the model will continue to remain on the GPU."),(0,r.yg)("p",null,"You can also directly use ",(0,r.yg)("inlineCode",{parentName:"p"},"worker.offload_states()")," and ",(0,r.yg)("inlineCode",{parentName:"p"},"worker.reload_states()")," for more direct control over offload and reload timing."),(0,r.yg)("h2",{id:"usage-example"},"Usage Example"),(0,r.yg)("p",null,"The following is an example of using offload/reload control in ",(0,r.yg)("inlineCode",{parentName:"p"},"rlvr_pipeline.py"),":"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},'# After the inference phase, manually offload reward model states\nif not self.pipeline_config.async_pipeline:\n    for reward_cluster in self.rewards.values():\n        reward_cluster.offload_states()\n\n# When computing reference model log probs, control whether to offload states\nif self.is_lora:\n    batch.meta_info["disable_adapter"] = True\n    batch.meta_info["is_offload_states"] = False\n    ref_log_probs = self.actor_train.compute_log_probs(batch, blocking=True)\nelse:\n    ref_log_probs = self.reference.compute_log_probs(batch, blocking=True)\n')),(0,r.yg)("h2",{id:"context-manager-support"},"Context Manager Support"),(0,r.yg)("p",null,"The ROLL framework also provides the ",(0,r.yg)("inlineCode",{parentName:"p"},"state_offload_manager")," context manager to simplify state management:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},"from roll.utils.context_managers import state_offload_manager\n\nwith state_offload_manager(strategy, metrics, metric_infix, is_offload_states=True):\n    # Execute operations that require GPU state within this context\n    yield\n")),(0,r.yg)("p",null,"This context manager automatically handles:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"Loading model states to GPU"),(0,r.yg)("li",{parentName:"ol"},"Executing operations"),(0,r.yg)("li",{parentName:"ol"},"Deciding whether to offload states to CPU based on the ",(0,r.yg)("inlineCode",{parentName:"li"},"is_offload_states")," parameter")),(0,r.yg)("h2",{id:"memory-monitoring"},"Memory Monitoring"),(0,r.yg)("p",null,"The framework also provides memory usage monitoring functionality:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},'from roll.utils.context_managers import log_gpu_memory_usage\n\n# Record GPU memory usage\nlog_gpu_memory_usage(head="model_loading", logger=logger, rank=None)\n')),(0,r.yg)("h2",{id:"usage-recommendations"},"Usage Recommendations"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},"In resource-constrained situations, properly using the offload/reload feature can significantly improve GPU utilization"),(0,r.yg)("li",{parentName:"ol"},"In pipeline implementation, arrange the execution order of different roles to maximize resource utilization efficiency, such as parallel computation of ref/reward models"),(0,r.yg)("li",{parentName:"ol"},"In asynchronous training, properly arrange the execution order of different roles to maximize resource utilization efficiency")))}p.isMDXComponent=!0}}]);