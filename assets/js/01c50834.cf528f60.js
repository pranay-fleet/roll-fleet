"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[1151],{5680:(e,n,r)=>{r.d(n,{xA:()=>g,yg:()=>y});var t=r(6540);function a(e,n,r){return n in e?Object.defineProperty(e,n,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[n]=r,e}function i(e,n){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);n&&(t=t.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),r.push.apply(r,t)}return r}function o(e){for(var n=1;n<arguments.length;n++){var r=null!=arguments[n]?arguments[n]:{};n%2?i(Object(r),!0).forEach(function(n){a(e,n,r[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(r,n))})}return e}function l(e,n){if(null==e)return{};var r,t,a=function(e,n){if(null==e)return{};var r,t,a={},i=Object.keys(e);for(t=0;t<i.length;t++)r=i[t],n.indexOf(r)>=0||(a[r]=e[r]);return a}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(t=0;t<i.length;t++)r=i[t],n.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(a[r]=e[r])}return a}var s=t.createContext({}),u=function(e){var n=t.useContext(s),r=n;return e&&(r="function"==typeof e?e(n):o(o({},n),e)),r},g=function(e){var n=u(e.components);return t.createElement(s.Provider,{value:n},e.children)},p="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return t.createElement(t.Fragment,{},n)}},c=t.forwardRef(function(e,n){var r=e.components,a=e.mdxType,i=e.originalType,s=e.parentName,g=l(e,["components","mdxType","originalType","parentName"]),p=u(r),c=a,y=p["".concat(s,".").concat(c)]||p[c]||m[c]||i;return r?t.createElement(y,o(o({ref:n},g),{},{components:r})):t.createElement(y,o({ref:n},g))});function y(e,n){var r=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var i=r.length,o=new Array(i);o[0]=c;var l={};for(var s in n)hasOwnProperty.call(n,s)&&(l[s]=n[s]);l.originalType=e,l[p]="string"==typeof e?e:a,o[1]=l;for(var u=2;u<i;u++)o[u]=r[u];return t.createElement.apply(null,o)}return t.createElement.apply(null,r)}c.displayName="MDXCreateElement"},8217:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>s,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>u});var t=r(8168),a=(r(6540),r(5680));const i={},o="Agentic Asynchronous Parallel Rollout",l={unversionedId:"English/UserGuide/agentic_async_parallel_rollout",id:"English/UserGuide/agentic_async_parallel_rollout",title:"Agentic Asynchronous Parallel Rollout",description:"Introduction",source:"@site/docs/English/UserGuide/agentic_async_parallel_rollout.md",sourceDirName:"English/UserGuide",slug:"/English/UserGuide/agentic_async_parallel_rollout",permalink:"/ROLL/docs/English/UserGuide/agentic_async_parallel_rollout",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/agentic_async_parallel_rollout.md",tags:[],version:"current",lastUpdatedAt:1761386293,formattedLastUpdatedAt:"Oct 25, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"TrajWiseLearning\u2014\u2014StarPO (State-Thinking-Actions-Reward Policy Optimization)",permalink:"/ROLL/docs/English/UserGuide/agentic/agentic_StarPO"},next:{title:"Group Relative Policy Optimization (GRPO)",permalink:"/ROLL/docs/English/UserGuide/algorithms/GRPO"}},s={},u=[{value:"Introduction",id:"introduction",level:2},{value:"Implementation Principle",id:"implementation-principle",level:2},{value:"Key Configuration Parameters",id:"key-configuration-parameters",level:2},{value:"Configuration Parameter Details",id:"configuration-parameter-details",level:3},{value:"max_env_num_per_worker",id:"max_env_num_per_worker",level:4},{value:"num_env_groups",id:"num_env_groups",level:4},{value:"group_size",id:"group_size",level:4},{value:"tags",id:"tags",level:4},{value:"num_groups_partition",id:"num_groups_partition",level:4},{value:"Usage Recommendations",id:"usage-recommendations",level:2}],g={toc:u},p="wrapper";function m({components:e,...n}){return(0,a.yg)(p,(0,t.A)({},g,n,{components:e,mdxType:"MDXLayout"}),(0,a.yg)("h1",{id:"agentic-asynchronous-parallel-rollout"},"Agentic Asynchronous Parallel Rollout"),(0,a.yg)("h2",{id:"introduction"},"Introduction"),(0,a.yg)("p",null,"Agentic asynchronous parallel rollout is an efficient multi-turn interaction processing mechanism in the ROLL framework. This mechanism manages multi-turn interaction processes at the environment (env) granularity, with each EnvManager independently executing ",(0,a.yg)("inlineCode",{parentName:"p"},"run_rollout_loop")," without synchronization barriers between environments, thus achieving efficient parallel processing."),(0,a.yg)("h2",{id:"implementation-principle"},"Implementation Principle"),(0,a.yg)("p",null,"The core implementation scheme of agentic asynchronous parallel rollout is as follows:"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Environment Granularity Management"),": Multi-turn interaction processes are managed at the env granularity, implemented in ",(0,a.yg)("inlineCode",{parentName:"li"},"roll/pipeline/agentic/env_manager/traj_env_manager.py")),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Independent Execution"),": Each EnvManager independently executes ",(0,a.yg)("inlineCode",{parentName:"li"},"run_rollout_loop")," without barriers between envs"),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Batch Processing"),": The ",(0,a.yg)("inlineCode",{parentName:"li"},"rollout_scheduler.get_batch()")," function in ",(0,a.yg)("inlineCode",{parentName:"li"},"AgenticPipeline")," blocks until the required ",(0,a.yg)("inlineCode",{parentName:"li"},"batch_size")," of trajectories is obtained")),(0,a.yg)("p",null,"The key difference between synchronous and asynchronous training lies in whether the EnvManager.run_rollout_loop() process needs to be paused after ",(0,a.yg)("inlineCode",{parentName:"p"},"rollout_scheduler.get_batch()")," returns:"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Synchronous Training"),": After collecting ",(0,a.yg)("inlineCode",{parentName:"li"},"batch_size")," trajectories, the rollout_loop exits"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Asynchronous Training"),": After collecting ",(0,a.yg)("inlineCode",{parentName:"li"},"batch_size")," trajectories, the pipeline continues with subsequent execution while continuing to execute EnvManager.run_rollout_loop")),(0,a.yg)("h2",{id:"key-configuration-parameters"},"Key Configuration Parameters"),(0,a.yg)("p",null,"In Agentic, the most core configuration is EnvManagerConfig, which describes the distribution information of various environment quantities. The key configuration parameters for EnvManager are as follows:"),(0,a.yg)("pre",null,(0,a.yg)("code",{parentName:"pre",className:"language-yaml"},"train_env_manager:\n  max_env_num_per_worker: 16\n  num_env_groups: 128\n  # Under the same group, the env config and env seed are ensured to be equal\n  group_size: 8\n  tags: [FrozenLake]\n  num_groups_partition: [128] # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation\n\nval_env_manager:\n  max_env_num_per_worker: 32\n  num_env_groups: 1024\n  group_size: 1 # Should be set to 1 because val temperature is set to 0 and same prompt leads to same output\n  tags: [SimpleSokoban, LargerSokoban, SokobanDifferentGridVocab, FrozenLake]\n  num_groups_partition: [256, 256, 256, 256] # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation\n")),(0,a.yg)("h3",{id:"configuration-parameter-details"},"Configuration Parameter Details"),(0,a.yg)("h4",{id:"max_env_num_per_worker"},"max_env_num_per_worker"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Meaning"),": The maximum number of environments that can run simultaneously per worker (Ray Actor)"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Purpose"),": Controls the concurrency of environments per single worker, affecting memory usage and parallelism"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Example"),": ",(0,a.yg)("inlineCode",{parentName:"li"},"max_env_num_per_worker: 16")," means each worker runs at most 16 environment instances simultaneously")),(0,a.yg)("h4",{id:"num_env_groups"},"num_env_groups"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Meaning"),": The total number of environment groups during training"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Purpose"),": Defines the total number of parallel environment groups, affecting training parallelism")),(0,a.yg)("h4",{id:"group_size"},"group_size"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Meaning"),": The number of environment instances contained in each environment group"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Purpose"),": Controls intra-group parallelism; environments within the same group have the same configuration and seed"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Notes"),":",(0,a.yg)("ul",{parentName:"li"},(0,a.yg)("li",{parentName:"ul"},"In training environments, typically set to a value greater than 1 to increase intra-group diversity"),(0,a.yg)("li",{parentName:"ul"},"In validation environments, should be set to 1 because validation temperature is 0, and identical prompts produce identical outputs"))),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Example"),":",(0,a.yg)("ul",{parentName:"li"},(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"group_size: 8")," means each environment group contains 8 environment instances"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"num_env_groups: 128")," means a total of 128 environment groups are created"),(0,a.yg)("li",{parentName:"ul"},"Total number of env instances is: ",(0,a.yg)("inlineCode",{parentName:"li"},"group_size * num_env_groups")," = 1024")))),(0,a.yg)("h4",{id:"tags"},"tags"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Meaning"),": List of environment tags used to identify and select environment types"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Purpose"),": Specifies the environment types to use; the framework loads corresponding environment implementations based on tags"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Example"),": ",(0,a.yg)("inlineCode",{parentName:"li"},"tags: [SimpleSokoban, FrozenLake]")," indicates using SimpleSokoban and FrozenLake environment types")),(0,a.yg)("h4",{id:"num_groups_partition"},"num_groups_partition"),(0,a.yg)("ul",null,(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Meaning"),": Group number allocation for different environment types"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Purpose"),": Specifies the allocation ratio of different environment types in the total environment groups"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Default Behavior"),": If not set, all environment names are equally divided into groups"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("strong",{parentName:"li"},"Example"),":",(0,a.yg)("ul",{parentName:"li"},(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"num_groups_partition: [128]")," means a single environment type occupies all 128 groups"),(0,a.yg)("li",{parentName:"ul"},(0,a.yg)("inlineCode",{parentName:"li"},"num_groups_partition: [256, 256, 256, 256]")," means four environment types each occupy 256 groups")))),(0,a.yg)("h2",{id:"usage-recommendations"},"Usage Recommendations"),(0,a.yg)("ol",null,(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Reasonable Parallelism Settings"),": Set ",(0,a.yg)("inlineCode",{parentName:"li"},"max_env_num_per_worker")," and ",(0,a.yg)("inlineCode",{parentName:"li"},"num_env_groups")," appropriately based on hardware resources (CPU, memory)"),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Environment Group Configuration"),": Increase ",(0,a.yg)("inlineCode",{parentName:"li"},"group_size")," during training to improve intra-group parallelism; set to 1 during validation, which is required for GRPO-like algorithms that calculate advantages based on group trajectories"),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Environment Type Allocation"),": Reasonably allocate training resources for different environment types through ",(0,a.yg)("inlineCode",{parentName:"li"},"tags")," and ",(0,a.yg)("inlineCode",{parentName:"li"},"num_groups_partition")),(0,a.yg)("li",{parentName:"ol"},(0,a.yg)("strong",{parentName:"li"},"Resource Monitoring"),": Monitor system resource usage to avoid resource exhaustion due to too many environment instances")),(0,a.yg)("p",null,"By properly configuring these parameters, you can fully leverage the performance advantages of agentic asynchronous parallel rollout and improve training efficiency for multi-turn interaction tasks."))}m.isMDXComponent=!0}}]);