"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[212],{5680:(e,n,t)=>{t.d(n,{xA:()=>g,yg:()=>d});var a=t(6540);function r(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function i(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),t.push.apply(t,a)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?i(Object(t),!0).forEach(function(n){r(e,n,t[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):i(Object(t)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))})}return e}function o(e,n){if(null==e)return{};var t,a,r=function(e,n){if(null==e)return{};var t,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||(r[t]=e[t]);return r}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)t=i[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var p=a.createContext({}),s=function(e){var n=a.useContext(p),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},g=function(e){var n=s(e.components);return a.createElement(p.Provider,{value:n},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},c=a.forwardRef(function(e,n){var t=e.components,r=e.mdxType,i=e.originalType,p=e.parentName,g=o(e,["components","mdxType","originalType","parentName"]),m=s(t),c=r,d=m["".concat(p,".").concat(c)]||m[c]||u[c]||i;return t?a.createElement(d,l(l({ref:n},g),{},{components:t})):a.createElement(d,l({ref:n},g))});function d(e,n){var t=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var i=t.length,l=new Array(i);l[0]=c;var o={};for(var p in n)hasOwnProperty.call(n,p)&&(o[p]=n[p]);o.originalType=e,o[m]="string"==typeof e?e:r,l[1]=o;for(var s=2;s<i;s++)l[s]=t[s];return a.createElement.apply(null,l)}return a.createElement.apply(null,t)}c.displayName="MDXCreateElement"},6771:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>p,contentTitle:()=>l,default:()=>u,frontMatter:()=>i,metadata:()=>o,toc:()=>s});var a=t(8168),r=(t(6540),t(5680));const i={},l="Group Sequence Policy Optimization (GSPO)",o={unversionedId:"English/UserGuide/algorithms/GSPO",id:"English/UserGuide/algorithms/GSPO",title:"Group Sequence Policy Optimization (GSPO)",description:"Introduction",source:"@site/docs/English/UserGuide/algorithms/GSPO.md",sourceDirName:"English/UserGuide/algorithms",slug:"/English/UserGuide/algorithms/GSPO",permalink:"/ROLL/docs/English/UserGuide/algorithms/GSPO",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/algorithms/GSPO.md",tags:[],version:"current",lastUpdatedAt:1761384892,formattedLastUpdatedAt:"Oct 25, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Group Relative Policy Optimization (GRPO)",permalink:"/ROLL/docs/English/UserGuide/algorithms/GRPO"},next:{title:"Lite PPO",permalink:"/ROLL/docs/English/UserGuide/algorithms/LitePPO"}},p={},s=[{value:"Introduction",id:"introduction",level:2},{value:"GSPO Configuration Parameters",id:"gspo-configuration-parameters",level:2},{value:"Core Parameter Descriptions",id:"core-parameter-descriptions",level:3},{value:"PPO Related Parameters",id:"ppo-related-parameters",level:3},{value:"Differences Between GSPO and GRPO",id:"differences-between-gspo-and-grpo",level:2},{value:"Reference Example",id:"reference-example",level:2},{value:"References",id:"references",level:2}],g={toc:s},m="wrapper";function u({components:e,...n}){return(0,r.yg)(m,(0,a.A)({},g,n,{components:e,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"group-sequence-policy-optimization-gspo"},"Group Sequence Policy Optimization (GSPO)"),(0,r.yg)("h2",{id:"introduction"},"Introduction"),(0,r.yg)("p",null,"Group Sequence Policy Optimization (GSPO) is a reinforcement learning algorithm proposed by Alibaba's Qwen team for training large language models",(0,r.yg)("sup",{parentName:"p",id:"fnref-1"},(0,r.yg)("a",{parentName:"sup",href:"#fn-1",className:"footnote-ref"},"1")),". GSPO works as follows:"),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Sequence-Level Optimization"),": Unlike algorithms such as GRPO, GSPO performs importance ratio calculation, reward assignment, and optimization at the sequence level rather than the token level."),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Group Sampling"),': For a given problem, the model generates multiple possible solutions, forming a "group" of outputs.'),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Reward Assignment"),": Each solution is evaluated and assigned a reward based on its correctness or quality."),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Baseline Calculation"),": The average reward of the group serves as the baseline."),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("strong",{parentName:"li"},"Policy Update"),": The model updates its parameters by comparing each solution's reward to the group baseline.")),(0,r.yg)("h2",{id:"gspo-configuration-parameters"},"GSPO Configuration Parameters"),(0,r.yg)("p",null,"In ROLL, the GSPO algorithm-specific configuration parameters are as follows:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-yaml"},'# GSPO related\nadv_estimator: "grpo"\nimportance_sampling: seq\nrollout_batch_size: 64  # prompt\nnum_return_sequences_in_group: 8\nprompt_length: 2048\nresponse_length: 4096\n\n# ppo related\nppo_epochs: 1\nuse_kl_loss: true\nkl_loss_coef: 0.001\nloss_agg_mode: "seq-mean-token-mean"\n\n# advantage\nwhiten_advantages: false\nadvantage_clip: 2.0\ndual_clip_loss: true\n# clip\nreward_clip: 10\n# normalize\nnorm_mean_type: ~\nnorm_std_type: ~\n\n# reward\nadd_token_level_kl: false\n')),(0,r.yg)("h3",{id:"core-parameter-descriptions"},"Core Parameter Descriptions"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"adv_estimator"),': Advantage estimator type, set to "reinforce"'),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"importance_sampling"),': Importance sampling method, set to "seq" for sequence-level sampling'),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"rollout_batch_size"),": Number of prompts per rollout_batch_size"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"num_return_sequences_in_group"),": Number of responses generated per prompt (group size), the total number of samples trained per pipeline step is (rollout_batch_size * num_return_sequences_in_group)"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"prompt_length"),": Maximum length of prompts"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"response_length"),": Maximum length of responses")),(0,r.yg)("h3",{id:"ppo-related-parameters"},"PPO Related Parameters"),(0,r.yg)("p",null,"The following parameters are common in PPO but also apply to GSPO:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"ppo_epochs"),": Number of optimization rounds per batch of samples"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"use_kl_loss"),": Whether to use KL divergence loss"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"kl_loss_coef"),": KL-loss coefficient"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"loss_agg_mode"),': Loss aggregation mode, default is "seq-mean-token-sum", Literal','["token-mean", "seq-mean-token-sum", "seq-mean-token-mean", "seq-mean-token-sum-norm"]'),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"whiten_advantages"),": Whether to whiten advantage values"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"advantage_clip"),": Advantage value clipping range"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"dual_clip_loss"),": Whether to use dual clipping loss"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"reward_clip"),": Reward value clipping range"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"norm_mean_type"),': Mean type for reward normalization: the options are "batch", "group", "running", or None; the default is None'),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"norm_std_type"),': Std type for reward normalization: the options are "batch", "group", "running", or None; the default is None'),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"add_token_level_kl"),": Whether to add token-level KL penalty")),(0,r.yg)("h2",{id:"differences-between-gspo-and-grpo"},"Differences Between GSPO and GRPO"),(0,r.yg)("p",null,"Main differences between GSPO and GRPO algorithms:"),(0,r.yg)("table",null,(0,r.yg)("thead",{parentName:"table"},(0,r.yg)("tr",{parentName:"thead"},(0,r.yg)("th",{parentName:"tr",align:null},"Comparison Dimension"),(0,r.yg)("th",{parentName:"tr",align:null},"GRPO (Group Relative Policy Optimization)"),(0,r.yg)("th",{parentName:"tr",align:null},"GSPO (Group Sequence Policy Optimization)"))),(0,r.yg)("tbody",{parentName:"table"},(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("strong",{parentName:"td"},"Optimization Granularity")),(0,r.yg)("td",{parentName:"tr",align:null},"Token-level optimization"),(0,r.yg)("td",{parentName:"tr",align:null},"Sequence-level optimization, consistent with reward calculation granularity")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("strong",{parentName:"td"},"Importance Ratio Calculation")),(0,r.yg)("td",{parentName:"tr",align:null},"Based on token-level probability ratio calculation, each token independently calculates importance weights"),(0,r.yg)("td",{parentName:"tr",align:null},"Based on sequence-level probability ratio calculation, using geometric averaging for smoothing, calculating the joint probability ratio for the entire sequence")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("strong",{parentName:"td"},"Mixture of Experts (MoE) Support")),(0,r.yg)("td",{parentName:"tr",align:null},"Unstable training in MoE models, requiring additional techniques to maintain expert activation consistency"),(0,r.yg)("td",{parentName:"tr",align:null},"Naturally supports MoE model training without additional techniques, as it only focuses on sequence-level likelihood")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("strong",{parentName:"td"},"Variance Control")),(0,r.yg)("td",{parentName:"tr",align:null},"Due to per-token importance weight calculation, high variance noise is easily introduced"),(0,r.yg)("td",{parentName:"tr",align:null},"Significantly reduces variance through sequence-level importance sampling and length normalization")),(0,r.yg)("tr",{parentName:"tbody"},(0,r.yg)("td",{parentName:"tr",align:null},(0,r.yg)("strong",{parentName:"td"},"Clipping Mechanism")),(0,r.yg)("td",{parentName:"tr",align:null},"Clipping at the token level, potentially leading to inconsistent gradient updates"),(0,r.yg)("td",{parentName:"tr",align:null},"Clipping at the sequence level, providing more consistent and stable gradient updates")))),(0,r.yg)("h2",{id:"reference-example"},"Reference Example"),(0,r.yg)("p",null,"You can refer to the following configuration file to set up GSPO training:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"./examples/docs_examples/example_gspo.yaml"))),(0,r.yg)("h2",{id:"references"},"References"),(0,r.yg)("p",null,"[1]",': Qwen Team. "Group Sequence Policy Optimization." arXiv preprint arXiv:2507.18071 (2025). ',(0,r.yg)("a",{parentName:"p",href:"https://arxiv.org/abs/2507.18071"},"https://arxiv.org/abs/2507.18071")))}u.isMDXComponent=!0}}]);