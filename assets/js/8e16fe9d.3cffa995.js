"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[2591],{1939:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>p,contentTitle:()=>l,default:()=>m,frontMatter:()=>t,metadata:()=>o,toc:()=>s});var i=a(8168),r=(a(6540),a(5680));const t={},l="RLVR Pipeline for VLM",o={unversionedId:"English/UserGuide/pipeline/vl_rlvr_pipeline_start",id:"English/UserGuide/pipeline/vl_rlvr_pipeline_start",title:"RLVR Pipeline for VLM",description:"Table of Contents",source:"@site/docs/English/UserGuide/pipeline/vl_rlvr_pipeline_start.md",sourceDirName:"English/UserGuide/pipeline",slug:"/English/UserGuide/pipeline/vl_rlvr_pipeline_start",permalink:"/ROLL/docs/English/UserGuide/pipeline/vl_rlvr_pipeline_start",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/pipeline/vl_rlvr_pipeline_start.md",tags:[],version:"current",lastUpdatedAt:1761386293,formattedLastUpdatedAt:"Oct 25, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"RLVR Pipeline",permalink:"/ROLL/docs/English/UserGuide/pipeline/rlvr_pipeline_start"},next:{title:"Trackers and Metrics",permalink:"/ROLL/docs/English/UserGuide/trackers_and_metrics"}},p={},s=[{value:"\u2728\ufe0fOverview",id:"\ufe0foverview",level:2},{value:"\u2728\ufe0fCore Components",id:"\ufe0fcore-components",level:2},{value:"Main Module (<code>RLVRVLMPipeline</code>)",id:"main-module-rlvrvlmpipeline",level:3},{value:"Configuration File (<code>RLVRConfig</code>)",id:"configuration-file-rlvrconfig",level:3},{value:"Reward Worker",id:"reward-worker",level:3},{value:"\u2728\ufe0fData Preparation",id:"\ufe0fdata-preparation",level:2},{value:"Data Format",id:"data-format",level:3},{value:"\u2728\ufe0fRunning the Pipeline",id:"\ufe0frunning-the-pipeline",level:2},{value:"Method 1: Using Python Launcher Script",id:"method-1-using-python-launcher-script",level:3},{value:"Method 2: Using Helper Shell Scripts",id:"method-2-using-helper-shell-scripts",level:3},{value:"\u2728\ufe0fStep-by-Step Example",id:"\ufe0fstep-by-step-example",level:2},{value:"Step 1: Configure Settings",id:"step-1-configure-settings",level:3},{value:"Step 2: Prepare Environment and Dependencies",id:"step-2-prepare-environment-and-dependencies",level:3},{value:"Step 3: Launch the Pipeline",id:"step-3-launch-the-pipeline",level:3},{value:"Step 4: Monitoring",id:"step-4-monitoring",level:3},{value:"Step 5: Outputs and Results",id:"step-5-outputs-and-results",level:3}],d={toc:s},g="wrapper";function m({components:e,...n}){return(0,r.yg)(g,(0,i.A)({},d,n,{components:e,mdxType:"MDXLayout"}),(0,r.yg)("h1",{id:"rlvr-pipeline-for-vlm"},"RLVR Pipeline for VLM"),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Table of Contents")),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#rlvr-pipeline-for-vlm"},"RLVR Pipeline for VLM"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#%EF%B8%8Foverview"},"\u2728\ufe0fOverview")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#%EF%B8%8Fcore-components"},"\u2728\ufe0fCore Components"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#main-module-rlvrvlmpipeline"},"Main Module (",(0,r.yg)("inlineCode",{parentName:"a"},"RLVRPipeline"),")")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#configuration-file-rlvrconfig"},"Configuration File (",(0,r.yg)("inlineCode",{parentName:"a"},"RLVRConfig"),")")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#reward-worker"},"Reward Worker")))),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#%EF%B8%8Fdata-preparation"},"\u2728\ufe0fData Preparation"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#data-format"},"Data Format")))),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#%EF%B8%8Frunning-the-pipeline"},"\u2728\ufe0fRunning the Pipeline"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#method-1-using-python-launcher-script"},"Method 1: Using Python Launcher Script")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#method-2-using-helper-shell-scripts"},"Method 2: Using Helper Shell Scripts")))),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#%EF%B8%8Fstep-by-step-example"},"\u2728\ufe0fStep-by-Step Example"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#step-1-configure-settings"},"Step 1: Configure Settings")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#step-2-prepare-environment-and-dependencies"},"Step 2: Prepare Environment and Dependencies")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#step-3-launch-the-pipeline"},"Step 3: Launch the Pipeline")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#step-4-monitoring"},"Step 4: Monitoring")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("a",{parentName:"li",href:"#step-5-outputs-and-results"},"Step 5: Outputs and Results"))))))),(0,r.yg)("hr",null),(0,r.yg)("h2",{id:"\ufe0foverview"},"\u2728\ufe0fOverview"),(0,r.yg)("p",null," RLVR pipeline for VLM shares the same advantages with its counterpart for LLM (",(0,r.yg)("a",{parentName:"p",href:"/ROLL/docs/English/UserGuide/pipeline/rlvr_pipeline_start"},"doc"),"), and it has built-in support for both visual reasoning and visual perception tasks, including math (for reasoning) and detection (for perception) currently, each equipped with specialized reward evaluation mechanisms, which enables simultaneous optimization of model capabilities across multiple domains such as math and detection."),(0,r.yg)("hr",null),(0,r.yg)("h2",{id:"\ufe0fcore-components"},"\u2728\ufe0fCore Components"),(0,r.yg)("h3",{id:"main-module-rlvrvlmpipeline"},"Main Module (",(0,r.yg)("inlineCode",{parentName:"h3"},"RLVRVLMPipeline"),")"),(0,r.yg)("p",null,(0,r.yg)("inlineCode",{parentName:"p"},"RLVRVLMPipeline")," (located in ",(0,r.yg)("inlineCode",{parentName:"p"},"roll/pipeline/rlvr/rlvr_vlm_pipeline.py"),") is the primary coordinator for the entire reinforcement learning process. It manages the complete training workflow, including:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},"Initializing and managing distributed workers (actor, critic, reference, and various reward workers)."),(0,r.yg)("li",{parentName:"ul"},"Coordinating data collection and processing."),(0,r.yg)("li",{parentName:"ul"},"Executing model training steps (e.g., PPO updates for actor and critic)."),(0,r.yg)("li",{parentName:"ul"},"Handling model synchronization and checkpoint saving."),(0,r.yg)("li",{parentName:"ul"},"Validation set evaluation."),(0,r.yg)("li",{parentName:"ul"},"Recording metrics and experiment tracking.")),(0,r.yg)("p",null,(0,r.yg)("strong",{parentName:"p"},"Source code"),": ",(0,r.yg)("inlineCode",{parentName:"p"},"roll/pipeline/rlvr/rlvr_vlm_pipeline.py"),", in which Qwen2.5-VL is supported directly"),(0,r.yg)("hr",null),(0,r.yg)("h3",{id:"configuration-file-rlvrconfig"},"Configuration File (",(0,r.yg)("inlineCode",{parentName:"h3"},"RLVRConfig"),")"),(0,r.yg)("p",null,"RLVR pipeline for VLM uses the same configuration file (",(0,r.yg)("inlineCode",{parentName:"p"},"RLVRConfig"),") with LLM, please refer to ",(0,r.yg)("a",{parentName:"p",href:"/ROLL/docs/English/UserGuide/pipeline/rlvr_pipeline_start"},"docs of RLVR Pipeline for LLM")," for configuration details. "),(0,r.yg)("p",null,"A configuration example can be found in ",(0,r.yg)("inlineCode",{parentName:"p"},"examples/qwen2.5-vl-7B-rlvr/rlvr_megatron.yaml"),", and the difference with LLM mainly exists in rewards settings which include visual specific reward and wold be introduced later."),(0,r.yg)("p",null,"   ",(0,r.yg)("strong",{parentName:"p"},"Reward Settings"),"\nThe ",(0,r.yg)("inlineCode",{parentName:"p"},"rewards")," section contains reward worker configurations for different domains:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"math")),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"worker_cls"),": Worker class name (e.g., ",(0,r.yg)("inlineCode",{parentName:"li"},"MathRuleRewardWorker"),")"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"tag_included"),": These tags use the reward domain for calculation."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"model_args"),": Reward model parameters"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"world_size"),": Number of workers"))),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"cv_detection")),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},"Similar configuration, but for evaluation of detection results")),(0,r.yg)("p",{parentName:"li"},"Note that the domains provided here (math and cv_detection) should be same as listed in ",(0,r.yg)("inlineCode",{parentName:"p"},"domain_interleave_probs")))),(0,r.yg)("hr",null),(0,r.yg)("h3",{id:"reward-worker"},"Reward Worker"),(0,r.yg)("p",null,"The VLM rlvr pipeline supports reward mechanisms for different rlvr domains, and the example used inlcudes:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Mathematical Rule Reward (",(0,r.yg)("inlineCode",{parentName:"strong"},"MathRuleRewardWorker"),")")," \u2013 Evaluates the correctness and steps of mathematical reasoning, which shares with RLVR pipeline for LLM")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Detection Reward (",(0,r.yg)("inlineCode",{parentName:"strong"},"DetectionRewardWorker"),")")," \u2013 Evaluates detection results."),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},"The detection verifier used here references to ",(0,r.yg)("a",{parentName:"li",href:"https://github.com/MiniMax-AI/One-RL-to-See-Them-All/blob/main/reward_server/verify.py"},"MiniMax-AI/One-RL-to-See-Them-All"),", which combines IoU and mAP scores with specified weighting coefficients for reward (a format reward is combined addtionally). "),(0,r.yg)("li",{parentName:"ul"},"For IoU score:",(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},"the IoU threshold is defined by ",(0,r.yg)("inlineCode",{parentName:"li"},"DET_IOU_THRESHOLD")," environment variable, and it can be set to one of ",(0,r.yg)("inlineCode",{parentName:"li"},"[0.5, 0.55, 0.6,0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 0.99]")," or set to one of ",(0,r.yg)("inlineCode",{parentName:"li"},"average")," and ",(0,r.yg)("inlineCode",{parentName:"li"},"dynamic"),". "),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"average")," is the default which averages IoU scores of all of these thresholds"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"dynamic")," is an introduced dynamic IoU reward mechanism that provides adaptive, progressive feedback to improves stability and performance. It uses different thresholds for different trainingsteps. Specifically, the IoU threshold is set to 0.85 for the first 10% training steps, and 0.95 for the next 15% training steps, and 0.99 for the remaining training steps."),(0,r.yg)("li",{parentName:"ul"},"Additionally, the IoU score for each threshold includes two items with differnet strategies(",(0,r.yg)("inlineCode",{parentName:"li"},"greedy_match_by_iou_max_iou_first")," and ",(0,r.yg)("inlineCode",{parentName:"li"},"greedy_match_by_iou_max_label_first"),"), and completeness (calculated by ",(0,r.yg)("inlineCode",{parentName:"li"},"1.0 - (FN_ratio + FP_raio) / 2.0"),") is also taken into account by a completeness weighting coefficient in each item score.")))),(0,r.yg)("p",{parentName:"li"},"Please refer to the ",(0,r.yg)("a",{parentName:"p",href:"https://arxiv.org/pdf/2505.18129"},"paper")," for more details"))),(0,r.yg)("hr",null),(0,r.yg)("h2",{id:"\ufe0fdata-preparation"},"\u2728\ufe0fData Preparation"),(0,r.yg)("h3",{id:"data-format"},"Data Format"),(0,r.yg)("p",null,"For multi-domain RLVR, we use data files with parquet format from ",(0,r.yg)("a",{parentName:"p",href:"https://huggingface.co/datasets/One-RL-to-See-Them-All/Orsta-Data-47k"},"One-RL-to-See-Them-All/Orsta-Data-47k")," as example inputs thus following their data schema, and here is a sample of math domain to illustrate the data format:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},'{\n    "data_source": "mm_math",\n    "images": [<PIL.PngImagePlugin.PngImageFile image mode=RGB size=252x56 at 0x15EEEA390>],\n    "prompt": [\n        {\n            "content": "<image>As shown in the figure, point D is the midpoint of line segment AC, $BC = \\\\frac{1}{2}AB$, and $BD = 1$cm. What is the length of AB?",\n            "role": "user",\n        }\n    ],\n    "ability": "math",\n    "reward_model": {\n        "answer": "4",\n        "ground_truth": "\\\\boxed{4}",\n        "accuracy_ratio": 1.0,\n        "format_ratio": 0.0,\n        "verifier": "mathverify",\n        "verifier_parm": {\n            "det_verifier_normalized": None,\n            "det_reward_ratio": {\n                "iou_max_label_first": None,\n                "iou_max_iou_first": None,\n                "iou_completeness": None,\n                "map": None,\n                "map50": None,\n                "map75": None,\n            },\n        },\n    },\n    "extra_info": {"id": None, "image_path": "images/51284809.png"},\n}\n')),(0,r.yg)("p",null,"The underlying data schema is"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},"{\n    'data_source': Value(dtype='string', id=None),\n    'images': Sequence(feature=Image(mode=None, decode=True, id=None), length=-1, id=None),\n    'prompt': [{'content': Value(dtype='string', id=None), 'role': Value(dtype='string', id=None)}],\n    'ability': Value(dtype='string', id=None),\n    'reward_model': {\n        'answer': Value(dtype='string', id=None),\n        'ground_truth': Value(dtype='string', id=None),\n        'accuracy_ratio': Value(dtype='float32', id=None),\n        'format_ratio': Value(dtype='float32', id=None),\n        'verifier': Value(dtype='string', id=None),\n        'verifier_parm': {\n            'det_verifier_normalized': Value(dtype='bool', id=None),\n            'det_reward_ratio': {\n                'iou_max_label_first': Value(dtype='float32', id=None),\n                'iou_max_iou_first': Value(dtype='float32', id=None),\n                'iou_completeness': Value(dtype='float32', id=None),\n                'map': Value(dtype='float32', id=None),\n                'map50': Value(dtype='float32', id=None),\n                'map75': Value(dtype='float32', id=None)\n            }\n        }\n    },\n    'extra_info': {'id': Value(dtype='string', id=None), 'image_path': Value(dtype='string', id=None)}\n}\n")),(0,r.yg)("p",null,"with the following field description:"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"data_source")," (Required): the source of the data, e.g. ",(0,r.yg)("inlineCode",{parentName:"li"},"object365_train"),". ",(0,r.yg)("strong",{parentName:"li"},"NOTE: It should be one value included in ",(0,r.yg)("inlineCode",{parentName:"strong"},"tag_included")," of a domain provided under rewards section in the configuration to indicate this sample belonging to the domain and using corresponding reward worker")," . Additionally, it is used in validation to give seperated metrics for different data sources."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"images")," (Required): a list of PIL images"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"prompt")," (Required): prompt with chat templates. ",(0,r.yg)("strong",{parentName:"li"},"NOTE"),": use ",(0,r.yg)("inlineCode",{parentName:"li"},"<image>")," as image token and ",(0,r.yg)("inlineCode",{parentName:"li"},"prompt")," should have image tokens with the same number as ",(0,r.yg)("inlineCode",{parentName:"li"},"images")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"ability")," (Optional): indicating data domain, e.g. ",(0,r.yg)("inlineCode",{parentName:"li"},"cv_detection"),". Data from same domain should have the same ",(0,r.yg)("inlineCode",{parentName:"li"},"ability")," value. While it is not used yet."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"reward_model")," (Required): reward related information, combined with reward  ",(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"answer")," (Optional): the ground truth, not used yet and ",(0,r.yg)("inlineCode",{parentName:"li"},"ground_truth")," is used instead"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"ground_truth")," (Required): the formated ground truth commonly, and the format is corresponding to answer extraction method in reward worker. For example, ",(0,r.yg)("inlineCode",{parentName:"li"},"ground_truth")," of detection should be included in ",(0,r.yg)("inlineCode",{parentName:"li"},'"<answer>"')," and ",(0,r.yg)("inlineCode",{parentName:"li"},'"</answer>"')," to be consistent with ",(0,r.yg)("inlineCode",{parentName:"li"},"detection_reward_worker.py")," which extracts answer using ",(0,r.yg)("inlineCode",{parentName:"li"},'"<answer>"')," and ",(0,r.yg)("inlineCode",{parentName:"li"},'"</answer>"')),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"accuracy_ratio")," (Required): coeficient of accuracy for score/reward calculation"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"format_ratio")," (Required): coeficient of format for score/reward calculation"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"verifier")," (Optional): verifier name, e.g. ",(0,r.yg)("inlineCode",{parentName:"li"},"detection"),", not used yet. "),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"verifier_parm")," (Optional): verifier parameters, all sub-fields are optional and only required for detection",(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"det_verifier_normalized"),": whether to normalize bounding box iou to height/width sized ",(0,r.yg)("inlineCode",{parentName:"li"},"1000"),", this is needed when ground truth is normalized"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"det_reward_ratio"),": weighting coeficients for various detection score combination",(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"iou_max_label_first"),": coeficient for iou score using ",(0,r.yg)("inlineCode",{parentName:"li"},"greedy_match_by_iou_max_iou_first")," as strategy"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"iou_max_iou_first"),": coeficient for iou score using ",(0,r.yg)("inlineCode",{parentName:"li"},"greedy_match_by_iou_max_label_first")," as strategy"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"iou_completeness"),": coeficient for iou completeness "),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"map"),": coeficient for map"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"map50"),": coeficient for map50"),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"map75"),": coeficient for map75"))))))),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"extra_info")," (Optional): extra information, not used yet")),(0,r.yg)("p",null,"A more complicated sample with all fields from detection domain is as following:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-python"},'{\n    "data_source": "v3det_train",\n    "images": [<PIL.PngImagePlugin.PngImageFile image mode=RGB size=799x533 at 0x15EF1DF90>],\n    "prompt": [\n        {\n            "content": "<image>\\nLocate all objects of the designated class present in the image:\\n- dog collar\\n- shiba dog\\n\\nBegin by clearly explaining your thought process enclosed within <think> and </think> tags. Afterward, present your final detection results enclosed within <answer> and </answer> tags.\\nFor example:\\n<think>\\nYour detailed reasoning process here.\\n</think>\\n<answer>\\n[{\'bbox_2d\': [x1,y1,x2,y2],\'label\': label_name}]\\n</answer>",\n            "role": "user",\n        }\n    ],\n    "ability": "cv_detection",\n    "reward_model": {\n        "answer": "[{\'bbox_2d\': [484, 227, 818, 998], \'label\': \'shiba dog\'}, {\'bbox_2d\': [106, 142, 473, 998], \'label\': \'shiba dog\'}, {\'bbox_2d\': [274, 468, 427, 652], \'label\': \'dog collar\'}, {\'bbox_2d\': [490, 522, 609, 611], \'label\': \'dog collar\'}]",\n        "ground_truth": "<answer>\\n[{\'bbox_2d\': [484, 227, 818, 998], \'label\': \'shiba dog\'}, {\'bbox_2d\': [106, 142, 473, 998], \'label\': \'shiba dog\'}, {\'bbox_2d\': [274, 468, 427, 652], \'label\': \'dog collar\'}, {\'bbox_2d\': [490, 522, 609, 611], \'label\': \'dog collar\'}]\\n</answer>",\n        "accuracy_ratio": 1.0,\n        "format_ratio": 0.10000000149011612,\n        "verifier": "detection",\n        "verifier_parm": {\n            "det_verifier_normalized": True,\n            "det_reward_ratio": {\n                "iou_max_label_first": 1.0,\n                "iou_max_iou_first": 0.0,\n                "iou_completeness": 0.30000001192092896,\n                "map": 0.0,\n                "map50": 0.0,\n                "map75": 0.0,\n            },\n        },\n    },\n    "extra_info": {"id": None, "image_path": "images/a00004438/19_1169_36968567106_1209f085a7_c.jpg"},\n}\n')),(0,r.yg)("p",null,"You can organize your own data as the above format and use it in the pipeline"),(0,r.yg)("hr",null),(0,r.yg)("h2",{id:"\ufe0frunning-the-pipeline"},"\u2728\ufe0fRunning the Pipeline"),(0,r.yg)("h3",{id:"method-1-using-python-launcher-script"},"Method 1: Using Python Launcher Script"),(0,r.yg)("p",null,"The primary method is to use the ",(0,r.yg)("inlineCode",{parentName:"p"},"examples/start_rlvr_vl_pipeline.py")," script. This script uses Hydra to load and manage configurations."),(0,r.yg)("ol",null,(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Select or Create a Configuration File"),(0,r.yg)("br",{parentName:"p"}),"\n","Start with an example YAML (e.g., ",(0,r.yg)("inlineCode",{parentName:"p"},"examples/qwen2.5-vl-7B-rlvr/rlvr_megatron.yaml"),") or create your own configuration.")),(0,r.yg)("li",{parentName:"ol"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Execute the Python Launcher Script")),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"# Make sure you are in the root directory of the ROLL (ScaleAligner) project\n# export PYTHONPATH=$(pwd):$PYTHONPATH\n\npython examples/start_rlvr_vl_pipeline.py \\\n       --config_path examples/qwen2.5-vl-7B-rlvr \\\n       --config_name rlvr_megatron\n")),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"--config_path")," \u2013 Directory containing your YAML configuration."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("inlineCode",{parentName:"li"},"--config_name")," \u2013 Filename (without ",(0,r.yg)("inlineCode",{parentName:"li"},".yaml"),").")))),(0,r.yg)("h3",{id:"method-2-using-helper-shell-scripts"},"Method 2: Using Helper Shell Scripts"),(0,r.yg)("p",null,"The ",(0,r.yg)("inlineCode",{parentName:"p"},"examples")," directory typically contains shell scripts that wrap the Python launcher (e.g., ",(0,r.yg)("inlineCode",{parentName:"p"},"run_rlvr_pipeline.sh"),")."),(0,r.yg)("p",null,"Example structure:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},'#!/bin/bash\n# Example: examples/qwen2.5-vl-7B-rlvr/run_rlvr_pipeline.sh\n\nCONFIG_NAME="rlvr_megatron"            # rlvr_megatron.yaml\nCONFIG_PATH="examples/qwen2.5-vl-7B-rlvr"\n\n# Set environment variables and other configurations\n\npython examples/start_rlvr_vl_pipeline.py \\\n       --config_path $CONFIG_PATH \\\n       --config_name $CONFIG_NAME \\\n       "$@"   # Pass any additional parameters\n')),(0,r.yg)("p",null,"Run using:"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"bash examples/qwen2.5-vl-7B-rlvr/run_rlvr_pipeline.sh\n")),(0,r.yg)("hr",null),(0,r.yg)("h2",{id:"\ufe0fstep-by-step-example"},"\u2728\ufe0fStep-by-Step Example"),(0,r.yg)("h3",{id:"step-1-configure-settings"},"Step 1: Configure Settings"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"File: ",(0,r.yg)("inlineCode",{parentName:"p"},"examples/qwen2.5-vl-7B-rlvr/rlvr_megatron.yaml"),(0,r.yg)("br",{parentName:"p"}),"\n","Key sections include ",(0,r.yg)("inlineCode",{parentName:"p"},"exp_name"),", ",(0,r.yg)("inlineCode",{parentName:"p"},"seed"),", ",(0,r.yg)("inlineCode",{parentName:"p"},"output_dir"),", model paths, ",(0,r.yg)("inlineCode",{parentName:"p"},"actor_train"),", ",(0,r.yg)("inlineCode",{parentName:"p"},"actor_infer"),", ",(0,r.yg)("inlineCode",{parentName:"p"},"reference"),", PPO parameters, and reward configurations.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"Pay special attention to these configuration sections:"),(0,r.yg)("ul",{parentName:"li"},(0,r.yg)("li",{parentName:"ul"},"Data configuration: ",(0,r.yg)("inlineCode",{parentName:"li"},"actor_train.data_args.file_name")," and ",(0,r.yg)("inlineCode",{parentName:"li"},"domain_interleave_probs")),(0,r.yg)("li",{parentName:"ul"},"Model configuration: ",(0,r.yg)("inlineCode",{parentName:"li"},"pretrain")," path"),(0,r.yg)("li",{parentName:"ul"},"Distributed strategies: ",(0,r.yg)("inlineCode",{parentName:"li"},"strategy_args")," and ",(0,r.yg)("inlineCode",{parentName:"li"},"device_mapping")," for each worker"),(0,r.yg)("li",{parentName:"ul"},"Reward configuration: Reward workers for different domains in the ",(0,r.yg)("inlineCode",{parentName:"li"},"rewards")," section")))),(0,r.yg)("h3",{id:"step-2-prepare-environment-and-dependencies"},"Step 2: Prepare Environment and Dependencies"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"Ensure all necessary dependencies are installed. NOTE: VLLM is the only supported inference engine for VLM pipeline currently, thus use the corresponding requirement files:"),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"pip install -r requirements_torch260_vllm.txt\n"))),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"Verify that all model paths in the configuration are accessible.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},"Prepare training and validation datasets, ensuring they conform to the data format requirements described above."))),(0,r.yg)("h3",{id:"step-3-launch-the-pipeline"},"Step 3: Launch the Pipeline"),(0,r.yg)("pre",null,(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"python examples/start_rlvr_vl_pipeline.py \\\n       --config_path examples/qwen2.5-vl-7B-rlvr \\\n       --config_name rlvr_megatron\n")),(0,r.yg)("h3",{id:"step-4-monitoring"},"Step 4: Monitoring"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Console Output")," \u2013 Observe Hydra, Ray, and pipeline logs.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"Log Files")," \u2013 Check the ",(0,r.yg)("inlineCode",{parentName:"p"},"logging_dir")," specified in the YAML.")),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("p",{parentName:"li"},(0,r.yg)("strong",{parentName:"p"},"TensorBoard")),(0,r.yg)("pre",{parentName:"li"},(0,r.yg)("code",{parentName:"pre",className:"language-bash"},"tensorboard --logdir <your_log_dir>\n")))),(0,r.yg)("h3",{id:"step-5-outputs-and-results"},"Step 5: Outputs and Results"),(0,r.yg)("ul",null,(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Trained Models")," \u2013 Checkpoints are saved in the ",(0,r.yg)("inlineCode",{parentName:"li"},"output_dir"),"."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Evaluation Metrics")," \u2013 Recorded in TensorBoard and the console."),(0,r.yg)("li",{parentName:"ul"},(0,r.yg)("strong",{parentName:"li"},"Generated Examples")," \u2013 The pipeline periodically outputs generated examples so you can visually assess model improvements.")),(0,r.yg)("hr",null),(0,r.yg)("p",null,(0,r.yg)("em",{parentName:"p"},"Happy experimenting!")))}m.isMDXComponent=!0},5680:(e,n,a)=>{a.d(n,{xA:()=>d,yg:()=>y});var i=a(6540);function r(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function t(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);n&&(i=i.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),a.push.apply(a,i)}return a}function l(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?t(Object(a),!0).forEach(function(n){r(e,n,a[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):t(Object(a)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))})}return e}function o(e,n){if(null==e)return{};var a,i,r=function(e,n){if(null==e)return{};var a,i,r={},t=Object.keys(e);for(i=0;i<t.length;i++)a=t[i],n.indexOf(a)>=0||(r[a]=e[a]);return r}(e,n);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);for(i=0;i<t.length;i++)a=t[i],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var p=i.createContext({}),s=function(e){var n=i.useContext(p),a=n;return e&&(a="function"==typeof e?e(n):l(l({},n),e)),a},d=function(e){var n=s(e.components);return i.createElement(p.Provider,{value:n},e.children)},g="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return i.createElement(i.Fragment,{},n)}},u=i.forwardRef(function(e,n){var a=e.components,r=e.mdxType,t=e.originalType,p=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),g=s(a),u=r,y=g["".concat(p,".").concat(u)]||g[u]||m[u]||t;return a?i.createElement(y,l(l({ref:n},d),{},{components:a})):i.createElement(y,l({ref:n},d))});function y(e,n){var a=arguments,r=n&&n.mdxType;if("string"==typeof e||r){var t=a.length,l=new Array(t);l[0]=u;var o={};for(var p in n)hasOwnProperty.call(n,p)&&(o[p]=n[p]);o.originalType=e,o[g]="string"==typeof e?e:r,l[1]=o;for(var s=2;s<t;s++)l[s]=a[s];return i.createElement.apply(null,l)}return i.createElement.apply(null,a)}u.displayName="MDXCreateElement"}}]);