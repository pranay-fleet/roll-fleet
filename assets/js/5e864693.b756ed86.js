"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[5605],{4924:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>g,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>l,toc:()=>s});var r=a(8168),t=(a(6540),a(5680));const i={},o="SGLang Inference Backend Configuration Guide",l={unversionedId:"English/UserGuide/backend/sglang",id:"English/UserGuide/backend/sglang",title:"SGLang Inference Backend Configuration Guide",description:"SGLang is a fast and easy-to-use inference engine, particularly suitable for inference tasks of large-scale language models. This document will provide detailed instructions on how to configure and use the SGLang inference backend in the ROLL framework.",source:"@site/docs/English/UserGuide/backend/sglang.md",sourceDirName:"English/UserGuide/backend",slug:"/English/UserGuide/backend/sglang",permalink:"/ROLL/docs/English/UserGuide/backend/sglang",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/backend/sglang.md",tags:[],version:"current",lastUpdatedAt:1761384892,formattedLastUpdatedAt:"Oct 25, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"Megatron Inference and Training Backend Configuration Guide",permalink:"/ROLL/docs/English/UserGuide/backend/megatron"},next:{title:"vLLM Inference Backend Configuration Guide",permalink:"/ROLL/docs/English/UserGuide/backend/vllm"}},g={},s=[{value:"SGLang Introduction",id:"sglang-introduction",level:2},{value:"Configuring SGLang Strategy",id:"configuring-sglang-strategy",level:2},{value:"Basic Configuration Example",id:"basic-configuration-example",level:3},{value:"Configuration Parameter Details",id:"configuration-parameter-details",level:3},{value:"Integration with Other Components",id:"integration-with-other-components",level:2},{value:"Performance Optimization Recommendations",id:"performance-optimization-recommendations",level:2},{value:"Notes",id:"notes",level:2}],c={toc:s},p="wrapper";function m({components:e,...n}){return(0,t.yg)(p,(0,r.A)({},c,n,{components:e,mdxType:"MDXLayout"}),(0,t.yg)("h1",{id:"sglang-inference-backend-configuration-guide"},"SGLang Inference Backend Configuration Guide"),(0,t.yg)("p",null,"SGLang is a fast and easy-to-use inference engine, particularly suitable for inference tasks of large-scale language models. This document will provide detailed instructions on how to configure and use the SGLang inference backend in the ROLL framework."),(0,t.yg)("h2",{id:"sglang-introduction"},"SGLang Introduction"),(0,t.yg)("p",null,"SGLang is a structured generation language specifically designed for inference of large language models. It provides efficient inference performance and flexible programming interfaces."),(0,t.yg)("h2",{id:"configuring-sglang-strategy"},"Configuring SGLang Strategy"),(0,t.yg)("p",null,"In the ROLL framework, SGLang inference strategy can be configured by setting ",(0,t.yg)("inlineCode",{parentName:"p"},"strategy_args")," in the YAML configuration file."),(0,t.yg)("h3",{id:"basic-configuration-example"},"Basic Configuration Example"),(0,t.yg)("p",null,"The following is a typical SGLang configuration example (from ",(0,t.yg)("inlineCode",{parentName:"p"},"examples/qwen3-30BA3B-rlvr_megatron/rlvr_config_sglang.yaml"),"):"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},"actor_infer:\n  model_args:\n    disable_gradient_checkpointing: true\n    dtype: bf16\n  generating_args:\n    max_new_tokens: ${response_length}\n    top_p: 0.99\n    top_k: 100\n    num_beams: 1\n    temperature: 0.99\n    num_return_sequences: ${num_return_sequences_in_group}\n  strategy_args:\n    strategy_name: sglang\n    strategy_config:\n      mem_fraction_static: 0.7\n      load_format: dummy\n  num_gpus_per_worker: 2\n  device_mapping: list(range(0,24))\n")),(0,t.yg)("h3",{id:"configuration-parameter-details"},"Configuration Parameter Details"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"strategy_name"),": Set to ",(0,t.yg)("inlineCode",{parentName:"p"},"sglang")," to use the SGLang inference backend")),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"strategy_config"),": SGLang-specific configuration parameters. For more SGLang configuration parameters, see the ",(0,t.yg)("a",{parentName:"p",href:"https://docs.sglang.ai/"},"official documentation"),". The strategy_config is passed through directly to SGLang."),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"mem_fraction_static"),": GPU memory utilization ratio for static memory such as model weights and KV cache",(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},"Increase this value if KV cache building fails"),(0,t.yg)("li",{parentName:"ul"},"Decrease this value if CUDA memory is insufficient"))),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"load_format"),": Format for loading model weights",(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},'Since the model will be "updated" at the beginning, this value can be set to ',(0,t.yg)("inlineCode",{parentName:"li"},"dummy")))))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"num_gpus_per_worker"),": Number of GPUs allocated per worker"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},"SGLang can utilize multiple GPUs for parallel inference"))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"device_mapping"),": Specify the list of GPU device IDs to use")),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"infer_batch_size"),": Batch size during inference"))),(0,t.yg)("h2",{id:"integration-with-other-components"},"Integration with Other Components"),(0,t.yg)("p",null,"In the above example, we can see:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("inlineCode",{parentName:"li"},"actor_infer")," uses SGLang as the inference backend"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("inlineCode",{parentName:"li"},"actor_train")," uses Megatron for training"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("inlineCode",{parentName:"li"},"reference")," uses Megatron for inference"),(0,t.yg)("li",{parentName:"ol"},"Reward models use different inference backends (such as ",(0,t.yg)("inlineCode",{parentName:"li"},"hf_infer"),")")),(0,t.yg)("p",null,"This design allows different components to choose the most suitable inference engine according to their needs."),(0,t.yg)("h2",{id:"performance-optimization-recommendations"},"Performance Optimization Recommendations"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Memory Management"),":"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},"Properly set the ",(0,t.yg)("inlineCode",{parentName:"li"},"mem_fraction_static")," parameter to balance performance and memory usage"),(0,t.yg)("li",{parentName:"ul"},"Monitor GPU memory usage to avoid memory overflow"))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Parallel Processing"),":"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},"Appropriately increase ",(0,t.yg)("inlineCode",{parentName:"li"},"num_gpus_per_worker")," to utilize multiple GPUs for model loading and parallel inference"),(0,t.yg)("li",{parentName:"ul"},"Adjust ",(0,t.yg)("inlineCode",{parentName:"li"},"device_mapping")," according to hardware configuration. The number of SGLang engines is ",(0,t.yg)("inlineCode",{parentName:"li"},"len(device_mapping) // num_gpus_per_worker")))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"Batch Processing Optimization"),":"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"infer_batch_size")," is not effective, as continuous batching is automatically performed"),(0,t.yg)("li",{parentName:"ul"},"Consider the impact of sequence length on batch size")))),(0,t.yg)("h2",{id:"notes"},"Notes"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"SGLang requires specific versions of dependency libraries, please ensure compatible versions are installed"),(0,t.yg)("li",{parentName:"ol"},"In resource-constrained environments, carefully balance resource allocation among different components"),(0,t.yg)("li",{parentName:"ol"},"Integration of SGLang with training frameworks like Megatron may require additional configuration")))}m.isMDXComponent=!0},5680:(e,n,a)=>{a.d(n,{xA:()=>c,yg:()=>d});var r=a(6540);function t(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function i(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),a.push.apply(a,r)}return a}function o(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?i(Object(a),!0).forEach(function(n){t(e,n,a[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))})}return e}function l(e,n){if(null==e)return{};var a,r,t=function(e,n){if(null==e)return{};var a,r,t={},i=Object.keys(e);for(r=0;r<i.length;r++)a=i[r],n.indexOf(a)>=0||(t[a]=e[a]);return t}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)a=i[r],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(t[a]=e[a])}return t}var g=r.createContext({}),s=function(e){var n=r.useContext(g),a=n;return e&&(a="function"==typeof e?e(n):o(o({},n),e)),a},c=function(e){var n=s(e.components);return r.createElement(g.Provider,{value:n},e.children)},p="mdxType",m={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},u=r.forwardRef(function(e,n){var a=e.components,t=e.mdxType,i=e.originalType,g=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),p=s(a),u=t,d=p["".concat(g,".").concat(u)]||p[u]||m[u]||i;return a?r.createElement(d,o(o({ref:n},c),{},{components:a})):r.createElement(d,o({ref:n},c))});function d(e,n){var a=arguments,t=n&&n.mdxType;if("string"==typeof e||t){var i=a.length,o=new Array(i);o[0]=u;var l={};for(var g in n)hasOwnProperty.call(n,g)&&(l[g]=n[g]);l.originalType=e,l[p]="string"==typeof e?e:t,o[1]=l;for(var s=2;s<i;s++)o[s]=a[s];return r.createElement.apply(null,o)}return r.createElement.apply(null,a)}u.displayName="MDXCreateElement"}}]);