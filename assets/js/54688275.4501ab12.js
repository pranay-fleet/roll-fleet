"use strict";(globalThis.webpackChunkdocs_roll=globalThis.webpackChunkdocs_roll||[]).push([[3020],{5680:(e,n,a)=>{a.d(n,{xA:()=>s,yg:()=>u});var r=a(6540);function t(e,n,a){return n in e?Object.defineProperty(e,n,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[n]=a,e}function i(e,n){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter(function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable})),a.push.apply(a,r)}return a}function l(e){for(var n=1;n<arguments.length;n++){var a=null!=arguments[n]?arguments[n]:{};n%2?i(Object(a),!0).forEach(function(n){t(e,n,a[n])}):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach(function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(a,n))})}return e}function o(e,n){if(null==e)return{};var a,r,t=function(e,n){if(null==e)return{};var a,r,t={},i=Object.keys(e);for(r=0;r<i.length;r++)a=i[r],n.indexOf(a)>=0||(t[a]=e[a]);return t}(e,n);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(r=0;r<i.length;r++)a=i[r],n.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(t[a]=e[a])}return t}var g=r.createContext({}),p=function(e){var n=r.useContext(g),a=n;return e&&(a="function"==typeof e?e(n):l(l({},n),e)),a},s=function(e){var n=p(e.components);return r.createElement(g.Provider,{value:n},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},m=r.forwardRef(function(e,n){var a=e.components,t=e.mdxType,i=e.originalType,g=e.parentName,s=o(e,["components","mdxType","originalType","parentName"]),c=p(a),m=t,u=c["".concat(g,".").concat(m)]||c[m]||d[m]||i;return a?r.createElement(u,l(l({ref:n},s),{},{components:a})):r.createElement(u,l({ref:n},s))});function u(e,n){var a=arguments,t=n&&n.mdxType;if("string"==typeof e||t){var i=a.length,l=new Array(i);l[0]=m;var o={};for(var g in n)hasOwnProperty.call(n,g)&&(o[g]=n[g]);o.originalType=e,o[c]="string"==typeof e?e:t,l[1]=o;for(var p=2;p<i;p++)l[p]=a[p];return r.createElement.apply(null,l)}return r.createElement.apply(null,a)}m.displayName="MDXCreateElement"},6863:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>g,contentTitle:()=>l,default:()=>d,frontMatter:()=>i,metadata:()=>o,toc:()=>p});var r=a(8168),t=(a(6540),a(5680));const i={},l="Megatron Inference and Training Backend Configuration Guide",o={unversionedId:"English/UserGuide/backend/megatron",id:"English/UserGuide/backend/megatron",title:"Megatron Inference and Training Backend Configuration Guide",description:"Megatron is NVIDIA's large-scale language model training and inference framework that supports efficient distributed training and inference. This document will provide detailed instructions on how to configure and use the Megatron backend in the ROLL framework.",source:"@site/docs/English/UserGuide/backend/megatron.md",sourceDirName:"English/UserGuide/backend",slug:"/English/UserGuide/backend/megatron",permalink:"/ROLL/docs/English/UserGuide/backend/megatron",draft:!1,editUrl:"https://github.com/alibaba/ROLL/tree/main/docs_roll/docs/English/UserGuide/backend/megatron.md",tags:[],version:"current",lastUpdatedAt:1761384892,formattedLastUpdatedAt:"Oct 25, 2025",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"LoRA Fine-tuning Configuration Guide",permalink:"/ROLL/docs/English/UserGuide/backend/lora"},next:{title:"SGLang Inference Backend Configuration Guide",permalink:"/ROLL/docs/English/UserGuide/backend/sglang"}},g={},p=[{value:"Megatron Introduction",id:"megatron-introduction",level:2},{value:"Configuring Megatron Strategy",id:"configuring-megatron-strategy",level:2},{value:"Training Configuration Example",id:"training-configuration-example",level:3},{value:"Inference Configuration Example",id:"inference-configuration-example",level:3},{value:"Configuration Parameter Details",id:"configuration-parameter-details",level:3},{value:"Integration with Other Components",id:"integration-with-other-components",level:2},{value:"Notes",id:"notes",level:2}],s={toc:p},c="wrapper";function d({components:e,...n}){return(0,t.yg)(c,(0,r.A)({},s,n,{components:e,mdxType:"MDXLayout"}),(0,t.yg)("h1",{id:"megatron-inference-and-training-backend-configuration-guide"},"Megatron Inference and Training Backend Configuration Guide"),(0,t.yg)("p",null,"Megatron is NVIDIA's large-scale language model training and inference framework that supports efficient distributed training and inference. This document will provide detailed instructions on how to configure and use the Megatron backend in the ROLL framework."),(0,t.yg)("h2",{id:"megatron-introduction"},"Megatron Introduction"),(0,t.yg)("p",null,"Megatron provides efficient model parallel and data parallel strategies, particularly suitable for training and inference of ultra-large-scale language models. It supports multiple parallel strategies including tensor parallelism, pipeline parallelism, and expert parallelism."),(0,t.yg)("h2",{id:"configuring-megatron-strategy"},"Configuring Megatron Strategy"),(0,t.yg)("p",null,"In the ROLL framework, Megatron training and inference strategies can be configured by setting ",(0,t.yg)("inlineCode",{parentName:"p"},"strategy_args")," in the YAML configuration file."),(0,t.yg)("h3",{id:"training-configuration-example"},"Training Configuration Example"),(0,t.yg)("p",null,"The following is a typical Megatron training configuration example (from ",(0,t.yg)("inlineCode",{parentName:"p"},"examples/qwen3-30BA3B-rlvr_megatron/rlvr_config_sglang.yaml"),"):"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},'actor_train:\n  model_args:\n    disable_gradient_checkpointing: false\n    dtype: bf16\n    model_type: ~\n  training_args:\n    learning_rate: 1.0e-6\n    weight_decay: 0\n    per_device_train_batch_size: 1\n    gradient_accumulation_steps: 32\n    warmup_steps: 20\n    num_train_epochs: 50\n  strategy_args:\n    strategy_name: megatron_train\n    strategy_config:\n      tensor_model_parallel_size: 2\n      pipeline_model_parallel_size: 2\n      virtual_pipeline_model_parallel_size: 8\n      expert_model_parallel_size: 4\n      context_parallel_size: 1\n      use_distributed_optimizer: true\n      sequence_parallel: true\n      moe_token_dispatcher_type: "alltoall"\n      moe_grouped_gemm: true\n      moe_layer_recompute: true\n  device_mapping: list(range(0,32))\n  infer_batch_size: 2\n')),(0,t.yg)("h3",{id:"inference-configuration-example"},"Inference Configuration Example"),(0,t.yg)("p",null,"The following is a typical Megatron inference configuration example:"),(0,t.yg)("pre",null,(0,t.yg)("code",{parentName:"pre",className:"language-yaml"},'reference:\n  model_args:\n    disable_gradient_checkpointing: true\n    dtype: bf16\n    model_type: ~\n  strategy_args:\n    strategy_name: megatron_infer\n    strategy_config:\n      tensor_model_parallel_size: 1\n      pipeline_model_parallel_size: 1\n      expert_model_parallel_size: 4\n      moe_token_dispatcher_type: "alltoall"\n      moe_grouped_gemm: true\n  device_mapping: list(range(0,32))\n  infer_batch_size: 2\n')),(0,t.yg)("h3",{id:"configuration-parameter-details"},"Configuration Parameter Details"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"strategy_name"),": "),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"megatron_train")," for training"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"megatron_infer")," for inference"))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"strategy_config"),": All parallel optimization configurations provided by mcore can be used. For detailed information, please refer to the usage of ",(0,t.yg)("a",{parentName:"p",href:"https://github.com/NVIDIA/Megatron-LM/blob/main/README.md"},"megatron"),". Here are some common Megatron configuration parameters:"),(0,t.yg)("ul",{parentName:"li"},(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"tensor_model_parallel_size"),": Tensor model parallelism degree, partitioning intra-layer computation and memory across multiple GPUs"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"pipeline_model_parallel_size"),": Pipeline model parallelism degree, assigning different layers of the model to different GPUs"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"virtual_pipeline_model_parallel_size"),": Virtual pipeline parallelism degree, used to improve pipeline efficiency"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"expert_model_parallel_size"),": Expert model parallelism degree, assigning different experts to different GPUs in MoE models"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"context_parallel_size"),": Context parallelism degree, used for processing ultra-long sequences"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"use_distributed_optimizer"),": Whether to use distributed optimizer"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"sequence_parallel"),": Whether to enable sequence parallel optimization"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"moe_token_dispatcher_type"),": Token dispatcher type in MoE models ('allgather' or 'alltoall')"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"moe_grouped_gemm"),": Whether to enable grouped GEMM for MoE experts"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"moe_layer_recompute"),": Whether to checkpoint MoE layers to save activation memory"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"recompute_granularity"),": Activation value recomputation granularity ('full' or 'selective')"),(0,t.yg)("li",{parentName:"ul"},(0,t.yg)("inlineCode",{parentName:"li"},"overlap_grad_reduce"),": Whether to overlap gradient All-reduce process with backward propagation computation in distributed optimizer"))),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"device_mapping"),": Specify the list of GPU device IDs to use")),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("p",{parentName:"li"},(0,t.yg)("strong",{parentName:"p"},"infer_batch_size"),": Batch size during inference"))),(0,t.yg)("h2",{id:"integration-with-other-components"},"Integration with Other Components"),(0,t.yg)("p",null,"In the configuration example, we can see:"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("inlineCode",{parentName:"li"},"actor_train")," uses Megatron for training"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("inlineCode",{parentName:"li"},"actor_infer")," may use other inference backends (such as vLLM or SGLang)"),(0,t.yg)("li",{parentName:"ol"},(0,t.yg)("inlineCode",{parentName:"li"},"reference")," uses Megatron for inference"),(0,t.yg)("li",{parentName:"ol"},"Reward models may use different inference backends")),(0,t.yg)("p",null,"This design allows different components to choose the most suitable backend according to their needs."),(0,t.yg)("h2",{id:"notes"},"Notes"),(0,t.yg)("ol",null,(0,t.yg)("li",{parentName:"ol"},"Megatron requires specific versions of dependency libraries, please ensure compatible versions are installed"),(0,t.yg)("li",{parentName:"ol"},"Parallel strategy settings need to be adjusted according to hardware configuration and model size"),(0,t.yg)("li",{parentName:"ol"},"In resource-constrained environments, carefully balance resource allocation among different components")),(0,t.yg)("p",null,"By properly configuring the Megatron backend, you can fully leverage the performance advantages of the ROLL framework in large-scale language model training and inference."))}d.isMDXComponent=!0}}]);